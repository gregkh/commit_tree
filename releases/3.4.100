commit 82f9c4a309835df86bddcb3051e392fccfb57fcf
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Mon Jul 28 07:07:01 2014 -0700

    Linux 3.4.100

commit 21870a3ce73bc351e2e3adffdea863e681556f2d
Author: Takao Indoh <indou.takao@jp.fujitsu.com>
Date:   Tue Apr 23 17:35:03 2013 +0900

    iommu/vt-d: Disable translation if already enabled
    
    commit 3a93c841c2b3b14824f7728dd74bd00a1cedb806 upstream.
    
    This patch disables translation(dma-remapping) before its initialization
    if it is already enabled.
    
    This is needed for kexec/kdump boot. If dma-remapping is enabled in the
    first kernel, it need to be disabled before initializing its page table
    during second kernel boot. Wei Hu also reported that this is needed
    when second kernel boots with intel_iommu=off.
    
    Basically iommu->gcmd is used to know whether translation is enabled or
    disabled, but it is always zero at boot time even when translation is
    enabled since iommu->gcmd is initialized without considering such a
    case. Therefor this patch synchronizes iommu->gcmd value with global
    command register when iommu structure is allocated.
    
    Signed-off-by: Takao Indoh <indou.takao@jp.fujitsu.com>
    Signed-off-by: Joerg Roedel <joro@8bytes.org>
    [wyj: Backported to 3.4: adjust context]
    Signed-off-by: Yijing Wang <wangyijing@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5dc1c8851364ffb79cb9403f72c712ee83cce755
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586 upstream.
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 299e667e26e9e3382fa471370121b117fa8ae987
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Jul 7 14:06:11 2014 -0700

    alarmtimer: Fix bug where relative alarm timers were treated as absolute
    
    commit 16927776ae757d0d132bdbfabbfe2c498342bd59 upstream.
    
    Sharvil noticed with the posix timer_settime interface, using the
    CLOCK_REALTIME_ALARM or CLOCK_BOOTTIME_ALARM clockid, if the users
    tried to specify a relative time timer, it would incorrectly be
    treated as absolute regardless of the state of the flags argument.
    
    This patch corrects this, properly checking the absolute/relative flag,
    as well as adds further error checking that no invalid flag bits are set.
    
    Reported-by: Sharvil Nanavati <sharvil@google.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Sharvil Nanavati <sharvil@google.com>
    Link: http://lkml.kernel.org/r/1404767171-6902-1-git-send-email-john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b63dd4c81b9eccf95c1d08775bb7f2d05d70c8f1
Author: Alex Deucher <alexander.deucher@amd.com>
Date:   Mon Jul 14 17:57:19 2014 -0400

    drm/radeon: avoid leaking edid data
    
    commit 0ac66effe7fcdee55bda6d5d10d3372c95a41920 upstream.
    
    In some cases we fetch the edid in the detect() callback
    in order to determine what sort of monitor is connected.
    If that happens, don't fetch the edid again in the get_modes()
    callback or we will leak the edid.
    
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ed379762fda2d66dd4907035c79467daafe7d70b
Author: Amitkumar Karwar <akarwar@marvell.com>
Date:   Fri Jun 20 11:45:25 2014 -0700

    mwifiex: fix Tx timeout issue
    
    commit d76744a93246eccdca1106037e8ee29debf48277 upstream.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=70191
    https://bugzilla.kernel.org/show_bug.cgi?id=77581
    
    It is observed that sometimes Tx packet is downloaded without
    adding driver's txpd header. This results in firmware parsing
    garbage data as packet length. Sometimes firmware is unable
    to read the packet if length comes out as invalid. This stops
    further traffic and timeout occurs.
    
    The root cause is uninitialized fields in tx_info(skb->cb) of
    packet used to get garbage values. In this case if
    MWIFIEX_BUF_FLAG_REQUEUED_PKT flag is mistakenly set, txpd
    header was skipped. This patch makes sure that tx_info is
    correctly initialized to fix the problem.
    
    Reported-by: Andrew Wiley <wiley.andrew.j@gmail.com>
    Reported-by: Linus Gasser <list@markas-al-nour.org>
    Reported-by: Michael Hirsch <hirsch@teufel.de>
    Tested-by: Xinming Hu <huxm@marvell.com>
    Signed-off-by: Amitkumar Karwar <akarwar@marvell.com>
    Signed-off-by: Maithili Hinge <maithili@marvell.com>
    Signed-off-by: Avinash Patil <patila@marvell.com>
    Signed-off-by: Bing Zhao <bzhao@marvell.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a2b2c0307e994401075d0cb255cd522cf6373080
Author: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
Date:   Wed Jun 25 10:09:07 2014 +0900

    perf/x86/intel: ignore CondChgd bit to avoid false NMI handling
    
    commit b292d7a10487aee6e74b1c18b8d95b92f40d4a4f upstream.
    
    Currently, any NMI is falsely handled by a NMI handler of NMI watchdog
    if CondChgd bit in MSR_CORE_PERF_GLOBAL_STATUS MSR is set.
    
    For example, we use external NMI to make system panic to get crash
    dump, but in this case, the external NMI is falsely handled do to the
    issue.
    
    This commit deals with the issue simply by ignoring CondChgd bit.
    
    Here is explanation in detail.
    
    On x86 NMI watchdog uses performance monitoring feature to
    periodically signal NMI each time performance counter gets overflowed.
    
    intel_pmu_handle_irq() is called as a NMI_LOCAL handler from a NMI
    handler of NMI watchdog, perf_event_nmi_handler(). It identifies an
    owner of a given NMI by looking at overflow status bits in
    MSR_CORE_PERF_GLOBAL_STATUS MSR. If some of the bits are set, then it
    handles the given NMI as its own NMI.
    
    The problem is that the intel_pmu_handle_irq() doesn't distinguish
    CondChgd bit from other bits. Unlike the other status bits, CondChgd
    bit doesn't represent overflow status for performance counters. Thus,
    CondChgd bit cannot be thought of as a mark indicating a given NMI is
    NMI watchdog's.
    
    As a result, if CondChgd bit is set, any NMI is falsely handled by the
    NMI handler of NMI watchdog. Also, if type of the falsely handled NMI
    is either NMI_UNKNOWN, NMI_SERR or NMI_IO_CHECK, the corresponding
    action is never performed until CondChgd bit is cleared.
    
    I noticed this behavior on systems with Ivy Bridge processors: Intel
    Xeon CPU E5-2630 v2 and Intel Xeon CPU E7-8890 v2. On both systems,
    CondChgd bit in MSR_CORE_PERF_GLOBAL_STATUS MSR has already been set
    in the beginning at boot. Then the CondChgd bit is immediately cleared
    by next wrmsr to MSR_CORE_PERF_GLOBAL_CTRL MSR and appears to remain
    0.
    
    On the other hand, on older processors such as Nehalem, Xeon E7540,
    CondChgd bit is not set in the beginning at boot.
    
    I'm not sure about exact behavior of CondChgd bit, in particular when
    this bit is set. Although I read Intel System Programmer's Manual to
    figure out that, the descriptions I found are:
    
      In 18.9.1:
    
      "The MSR_PERF_GLOBAL_STATUS MSR also provides a ¡sticky bit¢ to
       indicate changes to the state of performancmonitoring hardware"
    
      In Table 35-2 IA-32 Architectural MSRs
    
      63 CondChg: status bits of this register has changed.
    
    These are different from the bahviour I see on the actual system as I
    explained above.
    
    At least, I think ignoring CondChgd bit should be enough for NMI
    watchdog perspective.
    
    Signed-off-by: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20140625.103503.409316067.d.hatayama@jp.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 26adeae37fe36e3212e4fe6799fcdfc01c935347
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 21 07:17:42 2014 +0200

    ipv4: fix buffer overflow in ip_options_compile()
    
    [ Upstream commit 10ec9472f05b45c94db3c854d22581a20b97db41 ]
    
    There is a benign buffer overflow in ip_options_compile spotted by
    AddressSanitizer[1] :
    
    Its benign because we always can access one extra byte in skb->head
    (because header is followed by struct skb_shared_info), and in this case
    this byte is not even used.
    
    [28504.910798] ==================================================================
    [28504.912046] AddressSanitizer: heap-buffer-overflow in ip_options_compile
    [28504.913170] Read of size 1 by thread T15843:
    [28504.914026]  [<ffffffff81802f91>] ip_options_compile+0x121/0x9c0
    [28504.915394]  [<ffffffff81804a0d>] ip_options_get_from_user+0xad/0x120
    [28504.916843]  [<ffffffff8180dedf>] do_ip_setsockopt.isra.15+0x8df/0x1630
    [28504.918175]  [<ffffffff8180ec60>] ip_setsockopt+0x30/0xa0
    [28504.919490]  [<ffffffff8181e59b>] tcp_setsockopt+0x5b/0x90
    [28504.920835]  [<ffffffff8177462f>] sock_common_setsockopt+0x5f/0x70
    [28504.922208]  [<ffffffff817729c2>] SyS_setsockopt+0xa2/0x140
    [28504.923459]  [<ffffffff818cfb69>] system_call_fastpath+0x16/0x1b
    [28504.924722]
    [28504.925106] Allocated by thread T15843:
    [28504.925815]  [<ffffffff81804995>] ip_options_get_from_user+0x35/0x120
    [28504.926884]  [<ffffffff8180dedf>] do_ip_setsockopt.isra.15+0x8df/0x1630
    [28504.927975]  [<ffffffff8180ec60>] ip_setsockopt+0x30/0xa0
    [28504.929175]  [<ffffffff8181e59b>] tcp_setsockopt+0x5b/0x90
    [28504.930400]  [<ffffffff8177462f>] sock_common_setsockopt+0x5f/0x70
    [28504.931677]  [<ffffffff817729c2>] SyS_setsockopt+0xa2/0x140
    [28504.932851]  [<ffffffff818cfb69>] system_call_fastpath+0x16/0x1b
    [28504.934018]
    [28504.934377] The buggy address ffff880026382828 is located 0 bytes to the right
    [28504.934377]  of 40-byte region [ffff880026382800, ffff880026382828)
    [28504.937144]
    [28504.937474] Memory state around the buggy address:
    [28504.938430]  ffff880026382300: ........ rrrrrrrr rrrrrrrr rrrrrrrr
    [28504.939884]  ffff880026382400: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28504.941294]  ffff880026382500: .....rrr rrrrrrrr rrrrrrrr rrrrrrrr
    [28504.942504]  ffff880026382600: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28504.943483]  ffff880026382700: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28504.944511] >ffff880026382800: .....rrr rrrrrrrr rrrrrrrr rrrrrrrr
    [28504.945573]                         ^
    [28504.946277]  ffff880026382900: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28505.094949]  ffff880026382a00: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28505.096114]  ffff880026382b00: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28505.097116]  ffff880026382c00: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28505.098472]  ffff880026382d00: ffffffff rrrrrrrr rrrrrrrr rrrrrrrr
    [28505.099804] Legend:
    [28505.100269]  f - 8 freed bytes
    [28505.100884]  r - 8 redzone bytes
    [28505.101649]  . - 8 allocated bytes
    [28505.102406]  x=1..7 - x allocated bytes + (8-x) redzone bytes
    [28505.103637] ==================================================================
    
    [1] https://code.google.com/p/address-sanitizer/wiki/AddressSanitizerForKernel
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4427f9abd5547fa76a4b5ebeda0043fd9b0b01c7
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Mon Jul 21 00:06:48 2014 +0100

    dns_resolver: Null-terminate the right string
    
    [ Upstream commit 640d7efe4c08f06c4ae5d31b79bd8740e7f6790a ]
    
    *_result[len] is parsed as *(_result[len]) which is not at all what we
    want to touch here.
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Fixes: 84a7c0b1db1c ("dns_resolver: assure that dns_query() result is null-terminated")
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c304a23b4e46a1f2adb59f2e134162cfbbf35e4f
Author: Manuel Schölling <manuel.schoelling@gmx.de>
Date:   Sat Jun 7 23:57:25 2014 +0200

    dns_resolver: assure that dns_query() result is null-terminated
    
    [ Upstream commit 84a7c0b1db1c17d5ded8d3800228a608e1070b40 ]
    
    dns_query() credulously assumes that keys are null-terminated and
    returns a copy of a memory block that is off by one.
    
    Signed-off-by: Manuel Schölling <manuel.schoelling@gmx.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7670d47228460135a71d9d8c1f88543eba30988f
Author: Sowmini Varadhan <sowmini.varadhan@oracle.com>
Date:   Wed Jul 16 10:02:26 2014 -0400

    sunvnet: clean up objects created in vnet_new() on vnet_exit()
    
    [ Upstream commit a4b70a07ed12a71131cab7adce2ce91c71b37060 ]
    
    Nothing cleans up the objects created by
    vnet_new(), they are completely leaked.
    
    vnet_exit(), after doing the vio_unregister_driver() to clean
    up ports, should call a helper function that iterates over vnet_list
    and cleans up those objects. This includes unregister_netdevice()
    as well as free_netdev().
    
    Signed-off-by: Sowmini Varadhan <sowmini.varadhan@oracle.com>
    Acked-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Reviewed-by: Karl Volz <karl.volz@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 48b83dfd85c307282147fe604251046732fb7483
Author: Christoph Schulz <develop@kristov.de>
Date:   Sun Jul 13 00:53:15 2014 +0200

    net: pppoe: use correct channel MTU when using Multilink PPP
    
    [ Upstream commit a8a3e41c67d24eb12f9ab9680cbb85e24fcd9711 ]
    
    The PPP channel MTU is used with Multilink PPP when ppp_mp_explode() (see
    ppp_generic module) tries to determine how big a fragment might be. According
    to RFC 1661, the MTU excludes the 2-byte PPP protocol field, see the
    corresponding comment and code in ppp_mp_explode():
    
                    /*
                     * hdrlen includes the 2-byte PPP protocol field, but the
                     * MTU counts only the payload excluding the protocol field.
                     * (RFC1661 Section 2)
                     */
                    mtu = pch->chan->mtu - (hdrlen - 2);
    
    However, the pppoe module *does* include the PPP protocol field in the channel
    MTU, which is wrong as it causes the PPP payload to be 1-2 bytes too big under
    certain circumstances (one byte if PPP protocol compression is used, two
    otherwise), causing the generated Ethernet packets to be dropped. So the pppoe
    module has to subtract two bytes from the channel MTU. This error only
    manifests itself when using Multilink PPP, as otherwise the channel MTU is not
    used anywhere.
    
    In the following, I will describe how to reproduce this bug. We configure two
    pppd instances for multilink PPP over two PPPoE links, say eth2 and eth3, with
    a MTU of 1492 bytes for each link and a MRRU of 2976 bytes. (This MRRU is
    computed by adding the two link MTUs and subtracting the MP header twice, which
    is 4 bytes long.) The necessary pppd statements on both sides are "multilink
    mtu 1492 mru 1492 mrru 2976". On the client side, we additionally need "plugin
    rp-pppoe.so eth2" and "plugin rp-pppoe.so eth3", respectively; on the server
    side, we additionally need to start two pppoe-server instances to be able to
    establish two PPPoE sessions, one over eth2 and one over eth3. We set the MTU
    of the PPP network interface to the MRRU (2976) on both sides of the connection
    in order to make use of the higher bandwidth. (If we didn't do that, IP
    fragmentation would kick in, which we want to avoid.)
    
    Now we send a ICMPv4 echo request with a payload of 2948 bytes from client to
    server over the PPP link. This results in the following network packet:
    
       2948 (echo payload)
     +    8 (ICMPv4 header)
     +   20 (IPv4 header)
    ---------------------
       2976 (PPP payload)
    
    These 2976 bytes do not exceed the MTU of the PPP network interface, so the
    IP packet is not fragmented. Now the multilink PPP code in ppp_mp_explode()
    prepends one protocol byte (0x21 for IPv4), making the packet one byte bigger
    than the negotiated MRRU. So this packet would have to be divided in three
    fragments. But this does not happen as each link MTU is assumed to be two bytes
    larger. So this packet is diveded into two fragments only, one of size 1489 and
    one of size 1488. Now we have for that bigger fragment:
    
       1489 (PPP payload)
     +    4 (MP header)
     +    2 (PPP protocol field for the MP payload (0x3d))
     +    6 (PPPoE header)
    --------------------------
       1501 (Ethernet payload)
    
    This packet exceeds the link MTU and is discarded.
    
    If one configures the link MTU on the client side to 1501, one can see the
    discarded Ethernet frames with tcpdump running on the client. A
    
    ping -s 2948 -c 1 192.168.15.254
    
    leads to the smaller fragment that is correctly received on the server side:
    
    (tcpdump -vvvne -i eth3 pppoes and ppp proto 0x3d)
    52:54:00:ad:87:fd > 52:54:00:79:5c:d0, ethertype PPPoE S (0x8864),
      length 1514: PPPoE  [ses 0x3] MLPPP (0x003d), length 1494: seq 0x000,
      Flags [end], length 1492
    
    and to the bigger fragment that is not received on the server side:
    
    (tcpdump -vvvne -i eth2 pppoes and ppp proto 0x3d)
    52:54:00:70:9e:89 > 52:54:00:5d:6f:b0, ethertype PPPoE S (0x8864),
      length 1515: PPPoE  [ses 0x5] MLPPP (0x003d), length 1495: seq 0x000,
      Flags [begin], length 1493
    
    With the patch below, we correctly obtain three fragments:
    
    52:54:00:ad:87:fd > 52:54:00:79:5c:d0, ethertype PPPoE S (0x8864),
      length 1514: PPPoE  [ses 0x1] MLPPP (0x003d), length 1494: seq 0x000,
      Flags [begin], length 1492
    52:54:00:70:9e:89 > 52:54:00:5d:6f:b0, ethertype PPPoE S (0x8864),
      length 1514: PPPoE  [ses 0x1] MLPPP (0x003d), length 1494: seq 0x000,
      Flags [none], length 1492
    52:54:00:ad:87:fd > 52:54:00:79:5c:d0, ethertype PPPoE S (0x8864),
      length 27: PPPoE  [ses 0x1] MLPPP (0x003d), length 7: seq 0x000,
      Flags [end], length 5
    
    And the ICMPv4 echo request is successfully received at the server side:
    
    IP (tos 0x0, ttl 64, id 21925, offset 0, flags [DF], proto ICMP (1),
      length 2976)
        192.168.222.2 > 192.168.15.254: ICMP echo request, id 30530, seq 0,
          length 2956
    
    The bug was introduced in commit c9aa6895371b2a257401f59d3393c9f7ac5a8698
    ("[PPPOE]: Advertise PPPoE MTU") from the very beginning. This patch applies
    to 3.10 upwards but the fix can be applied (with minor modifications) to
    kernels as old as 2.6.32.
    
    Signed-off-by: Christoph Schulz <develop@kristov.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9de2be635e77b3591e698d6a08abad98fb21693d
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Jul 12 20:30:35 2014 +0200

    net: sctp: fix information leaks in ulpevent layer
    
    [ Upstream commit 8f2e5ae40ec193bc0a0ed99e95315c3eebca84ea ]
    
    While working on some other SCTP code, I noticed that some
    structures shared with user space are leaking uninitialized
    stack or heap buffer. In particular, struct sctp_sndrcvinfo
    has a 2 bytes hole between .sinfo_flags and .sinfo_ppid that
    remains unfilled by us in sctp_ulpevent_read_sndrcvinfo() when
    putting this into cmsg. But also struct sctp_remote_error
    contains a 2 bytes hole that we don't fill but place into a skb
    through skb_copy_expand() via sctp_ulpevent_make_remote_error().
    
    Both structures are defined by the IETF in RFC6458:
    
    * Section 5.3.2. SCTP Header Information Structure:
    
      The sctp_sndrcvinfo structure is defined below:
    
      struct sctp_sndrcvinfo {
        uint16_t sinfo_stream;
        uint16_t sinfo_ssn;
        uint16_t sinfo_flags;
        <-- 2 bytes hole  -->
        uint32_t sinfo_ppid;
        uint32_t sinfo_context;
        uint32_t sinfo_timetolive;
        uint32_t sinfo_tsn;
        uint32_t sinfo_cumtsn;
        sctp_assoc_t sinfo_assoc_id;
      };
    
    * 6.1.3. SCTP_REMOTE_ERROR:
    
      A remote peer may send an Operation Error message to its peer.
      This message indicates a variety of error conditions on an
      association. The entire ERROR chunk as it appears on the wire
      is included in an SCTP_REMOTE_ERROR event. Please refer to the
      SCTP specification [RFC4960] and any extensions for a list of
      possible error formats. An SCTP error notification has the
      following format:
    
      struct sctp_remote_error {
        uint16_t sre_type;
        uint16_t sre_flags;
        uint32_t sre_length;
        uint16_t sre_error;
        <-- 2 bytes hole  -->
        sctp_assoc_t sre_assoc_id;
        uint8_t  sre_data[];
      };
    
    Fix this by setting both to 0 before filling them out. We also
    have other structures shared between user and kernel space in
    SCTP that contains holes (e.g. struct sctp_paddrthlds), but we
    copy that buffer over from user space first and thus don't need
    to care about it in that cases.
    
    While at it, we can also remove lengthy comments copied from
    the draft, instead, we update the comment with the correct RFC
    number where one can look it up.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d1558f713d5a4963f1bf95573bc870b816651f91
Author: Jon Paul Maloy <jon.maloy@ericsson.com>
Date:   Fri Jul 11 08:45:27 2014 -0400

    tipc: clear 'next'-pointer of message fragments before reassembly
    
    [ Upstream commit 999417549c16dd0e3a382aa9f6ae61688db03181 ]
    
    If the 'next' pointer of the last fragment buffer in a message is not
    zeroed before reassembly, we risk ending up with a corrupt message,
    since the reassembly function itself isn't doing this.
    
    Currently, when a buffer is retrieved from the deferred queue of the
    broadcast link, the next pointer is not cleared, with the result as
    described above.
    
    This commit corrects this, and thereby fixes a bug that may occur when
    long broadcast messages are transmitted across dual interfaces. The bug
    has been present since 40ba3cdf542a469aaa9083fa041656e59b109b90 ("tipc:
    message reassembly using fragment chain")
    
    This commit should be applied to both net and net-next.
    
    Signed-off-by: Jon Maloy <jon.maloy@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3c656d481c5ed4285a955fcd3e8e8779d664a30e
Author: Suresh Reddy <Suresh.Reddy@emulex.com>
Date:   Fri Jul 11 14:03:01 2014 +0530

    be2net: set EQ DB clear-intr bit in be_open()
    
    [ Upstream commit 4cad9f3b61c7268fa89ab8096e23202300399b5d ]
    
    On BE3, if the clear-interrupt bit of the EQ doorbell is not set the first
    time it is armed, ocassionally we have observed that the EQ doesn't raise
    anymore interrupts even if it is in armed state.
    This patch fixes this by setting the clear-interrupt bit when EQs are
    armed for the first time in be_open().
    
    Signed-off-by: Suresh Reddy <Suresh.Reddy@emulex.com>
    Signed-off-by: Sathya Perla <sathya.perla@emulex.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3fa1f507bc20f61bc8ec94139b62a113b0bf1a50
Author: Andrey Utkin <andrey.krieger.utkin@gmail.com>
Date:   Mon Jul 7 23:22:50 2014 +0300

    appletalk: Fix socket referencing in skb
    
    [ Upstream commit 36beddc272c111689f3042bf3d10a64d8a805f93 ]
    
    Setting just skb->sk without taking its reference and setting a
    destructor is invalid. However, in the places where this was done, skb
    is used in a way not requiring skb->sk setting. So dropping the setting
    of skb->sk.
    Thanks to Eric Dumazet <eric.dumazet@gmail.com> for correct solution.
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=79441
    Reported-by: Ed Martin <edman007@edman007.com>
    Signed-off-by: Andrey Utkin <andrey.krieger.utkin@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f37e491349d9dae99f2386585873abb1551ead55
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Jul 2 12:07:16 2014 -0700

    tcp: fix false undo corner cases
    
    [ Upstream commit 6e08d5e3c8236e7484229e46fdf92006e1dd4c49 ]
    
    The undo code assumes that, upon entering loss recovery, TCP
    1) always retransmit something
    2) the retransmission never fails locally (e.g., qdisc drop)
    
    so undo_marker is set in tcp_enter_recovery() and undo_retrans is
    incremented only when tcp_retransmit_skb() is successful.
    
    When the assumption is broken because TCP's cwnd is too small to
    retransmit or the retransmit fails locally. The next (DUP)ACK
    would incorrectly revert the cwnd and the congestion state in
    tcp_try_undo_dsack() or tcp_may_undo(). Subsequent (DUP)ACKs
    may enter the recovery state. The sender repeatedly enter and
    (incorrectly) exit recovery states if the retransmits continue to
    fail locally while receiving (DUP)ACKs.
    
    The fix is to initialize undo_retrans to -1 and start counting on
    the first retransmission. Always increment undo_retrans even if the
    retransmissions fail locally because they couldn't cause DSACKs to
    undo the cwnd reduction.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fbaf3a0b58bbdb2f26060689da8ac0820649d53a
Author: dingtianhong <dingtianhong@huawei.com>
Date:   Wed Jul 2 13:50:48 2014 +0800

    igmp: fix the problem when mc leave group
    
    [ Upstream commit 52ad353a5344f1f700c5b777175bdfa41d3cd65a ]
    
    The problem was triggered by these steps:
    
    1) create socket, bind and then setsockopt for add mc group.
       mreq.imr_multiaddr.s_addr = inet_addr("255.0.0.37");
       mreq.imr_interface.s_addr = inet_addr("192.168.1.2");
       setsockopt(sockfd, IPPROTO_IP, IP_ADD_MEMBERSHIP, &mreq, sizeof(mreq));
    
    2) drop the mc group for this socket.
       mreq.imr_multiaddr.s_addr = inet_addr("255.0.0.37");
       mreq.imr_interface.s_addr = inet_addr("0.0.0.0");
       setsockopt(sockfd, IPPROTO_IP, IP_DROP_MEMBERSHIP, &mreq, sizeof(mreq));
    
    3) and then drop the socket, I found the mc group was still used by the dev:
    
       netstat -g
    
       Interface       RefCnt Group
       --------------- ------ ---------------------
       eth2            1      255.0.0.37
    
    Normally even though the IP_DROP_MEMBERSHIP return error, the mc group still need
    to be released for the netdev when drop the socket, but this process was broken when
    route default is NULL, the reason is that:
    
    The ip_mc_leave_group() will choose the in_dev by the imr_interface.s_addr, if input addr
    is NULL, the default route dev will be chosen, then the ifindex is got from the dev,
    then polling the inet->mc_list and return -ENODEV, but if the default route dev is NULL,
    the in_dev and ifIndex is both NULL, when polling the inet->mc_list, the mc group will be
    released from the mc_list, but the dev didn't dec the refcnt for this mc group, so
    when dropping the socket, the mc_list is NULL and the dev still keep this group.
    
    v1->v2: According Hideaki's suggestion, we should align with IPv6 (RFC3493) and BSDs,
            so I add the checking for the in_dev before polling the mc_list, make sure when
            we remove the mc group, dec the refcnt to the real dev which was using the mc address.
            The problem would never happened again.
    
    Signed-off-by: Ding Tianhong <dingtianhong@huawei.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d7f54732a3b231407417caf3c2cf08b8bceaa34a
Author: Li RongQing <roy.qing.li@gmail.com>
Date:   Wed Jun 18 13:46:02 2014 +0800

    8021q: fix a potential memory leak
    
    [ Upstream commit 916c1689a09bc1ca81f2d7a34876f8d35aadd11b ]
    
    skb_cow called in vlan_reorder_header does not free the skb when it failed,
    and vlan_reorder_header returns NULL to reset original skb when it is called
    in vlan_untag, lead to a memory leak.
    
    Signed-off-by: Li RongQing <roy.qing.li@gmail.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 06c84e9853a3be378e67fa06c528c805cd43468f
Author: Neal Cardwell <ncardwell@google.com>
Date:   Wed Jun 18 21:15:03 2014 -0400

    tcp: fix tcp_match_skb_to_sack() for unaligned SACK at end of an skb
    
    [ Upstream commit 2cd0d743b05e87445c54ca124a9916f22f16742e ]
    
    If there is an MSS change (or misbehaving receiver) that causes a SACK
    to arrive that covers the end of an skb but is less than one MSS, then
    tcp_match_skb_to_sack() was rounding up pkt_len to the full length of
    the skb ("Round if necessary..."), then chopping all bytes off the skb
    and creating a zero-byte skb in the write queue.
    
    This was visible now because the recently simplified TLP logic in
    bef1909ee3ed1c ("tcp: fixing TLP's FIN recovery") could find that 0-byte
    skb at the end of the write queue, and now that we do not check that
    skb's length we could send it as a TLP probe.
    
    Consider the following example scenario:
    
     mss: 1000
     skb: seq: 0 end_seq: 4000  len: 4000
     SACK: start_seq: 3999 end_seq: 4000
    
    The tcp_match_skb_to_sack() code will compute:
    
     in_sack = false
     pkt_len = start_seq - TCP_SKB_CB(skb)->seq = 3999 - 0 = 3999
     new_len = (pkt_len / mss) * mss = (3999/1000)*1000 = 3000
     new_len += mss = 4000
    
    Previously we would find the new_len > skb->len check failing, so we
    would fall through and set pkt_len = new_len = 4000 and chop off
    pkt_len of 4000 from the 4000-byte skb, leaving a 0-byte segment
    afterward in the write queue.
    
    With this new commit, we notice that the new new_len >= skb->len check
    succeeds, so that we return without trying to fragment.
    
    Fixes: adb92db857ee ("tcp: Make SACK code to split only at mss boundaries")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Ilpo Jarvinen <ilpo.jarvinen@helsinki.fi>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 21618a8f0f46ff327acd6a079bc05c065561e4c0
Author: Hugh Dickins <hughd@google.com>
Date:   Wed Jul 23 14:00:13 2014 -0700

    shmem: fix splicing from a hole while it's punched
    
    commit b1a366500bd537b50c3aad26dc7df083ec03a448 upstream.
    
    shmem_fault() is the actual culprit in trinity's hole-punch starvation,
    and the most significant cause of such problems: since a page faulted is
    one that then appears page_mapped(), needing unmap_mapping_range() and
    i_mmap_mutex to be unmapped again.
    
    But it is not the only way in which a page can be brought into a hole in
    the radix_tree while that hole is being punched; and Vlastimil's testing
    implies that if enough other processors are busy filling in the hole,
    then shmem_undo_range() can be kept from completing indefinitely.
    
    shmem_file_splice_read() is the main other user of SGP_CACHE, which can
    instantiate shmem pagecache pages in the read-only case (without holding
    i_mutex, so perhaps concurrently with a hole-punch).  Probably it's
    silly not to use SGP_READ already (using the ZERO_PAGE for holes): which
    ought to be safe, but might bring surprises - not a change to be rushed.
    
    shmem_read_mapping_page_gfp() is an internal interface used by
    drivers/gpu/drm GEM (and next by uprobes): it should be okay.  And
    shmem_file_read_iter() uses the SGP_DIRTY variant of SGP_CACHE, when
    called internally by the kernel (perhaps for a stacking filesystem,
    which might rely on holes to be reserved): it's unclear whether it could
    be provoked to keep hole-punch busy or not.
    
    We could apply the same umbrella as now used in shmem_fault() to
    shmem_file_splice_read() and the others; but it looks ugly, and use over
    a range raises questions - should it actually be per page? can these get
    starved themselves?
    
    The origin of this part of the problem is my v3.1 commit d0823576bf4b
    ("mm: pincer in truncate_inode_pages_range"), once it was duplicated
    into shmem.c.  It seemed like a nice idea at the time, to ensure
    (barring RCU lookup fuzziness) that there's an instant when the entire
    hole is empty; but the indefinitely repeated scans to ensure that make
    it vulnerable.
    
    Revert that "enhancement" to hole-punch from shmem_undo_range(), but
    retain the unproblematic rescanning when it's truncating; add a couple
    of comments there.
    
    Remove the "indices[0] >= end" test: that is now handled satisfactorily
    by the inner loop, and mem_cgroup_uncharge_start()/end() are too light
    to be worth avoiding here.
    
    But if we do not always loop indefinitely, we do need to handle the case
    of swap swizzled back to page before shmem_free_swap() gets it: add a
    retry for that case, as suggested by Konstantin Khlebnikov; and for the
    case of page swizzled back to swap, as suggested by Johannes Weiner.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Suggested-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Lukas Czerner <lczerner@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0f5a4a003f5d440c09f414f0c6fc6c591baa8b15
Author: Hugh Dickins <hughd@google.com>
Date:   Wed Jul 23 14:00:10 2014 -0700

    shmem: fix faulting into a hole, not taking i_mutex
    
    commit 8e205f779d1443a94b5ae81aa359cb535dd3021e upstream.
    
    Commit f00cdc6df7d7 ("shmem: fix faulting into a hole while it's
    punched") was buggy: Sasha sent a lockdep report to remind us that
    grabbing i_mutex in the fault path is a no-no (write syscall may already
    hold i_mutex while faulting user buffer).
    
    We tried a completely different approach (see following patch) but that
    proved inadequate: good enough for a rational workload, but not good
    enough against trinity - which forks off so many mappings of the object
    that contention on i_mmap_mutex while hole-puncher holds i_mutex builds
    into serious starvation when concurrent faults force the puncher to fall
    back to single-page unmap_mapping_range() searches of the i_mmap tree.
    
    So return to the original umbrella approach, but keep away from i_mutex
    this time.  We really don't want to bloat every shmem inode with a new
    mutex or completion, just to protect this unlikely case from trinity.
    So extend the original with wait_queue_head on stack at the hole-punch
    end, and wait_queue item on the stack at the fault end.
    
    This involves further use of i_lock to guard against the races: lockdep
    has been happy so far, and I see fs/inode.c:unlock_new_inode() holds
    i_lock around wake_up_bit(), which is comparable to what we do here.
    i_lock is more convenient, but we could switch to shmem's info->lock.
    
    This issue has been tagged with CVE-2014-4171, which will require commit
    f00cdc6df7d7 and this and the following patch to be backported: we
    suggest to 3.1+, though in fact the trinity forkbomb effect might go
    back as far as 2.6.16, when madvise(,,MADV_REMOVE) came in - or might
    not, since much has changed, with i_mmap_mutex a spinlock before 3.0.
    Anyone running trinity on 3.0 and earlier? I don't think we need care.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Lukas Czerner <lczerner@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a62f374abb72830e1319d45ecb5e597b1944c8cd
Author: Hugh Dickins <hughd@google.com>
Date:   Mon Jun 23 13:22:06 2014 -0700

    shmem: fix faulting into a hole while it's punched
    
    commit f00cdc6df7d7cfcabb5b740911e6788cb0802bdb upstream.
    
    Trinity finds that mmap access to a hole while it's punched from shmem
    can prevent the madvise(MADV_REMOVE) or fallocate(FALLOC_FL_PUNCH_HOLE)
    from completing, until the reader chooses to stop; with the puncher's
    hold on i_mutex locking out all other writers until it can complete.
    
    It appears that the tmpfs fault path is too light in comparison with its
    hole-punching path, lacking an i_data_sem to obstruct it; but we don't
    want to slow down the common case.
    
    Extend shmem_fallocate()'s existing range notification mechanism, so
    shmem_fault() can refrain from faulting pages into the hole while it's
    punched, waiting instead on i_mutex (when safe to sleep; or repeatedly
    faulting when not).
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Dave Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 64fb4e2b0f802ac31f7350fbcba01de67845d4fb
Author: Markus F.X.J. Oberhumer <markus@oberhumer.com>
Date:   Sun Oct 14 15:39:04 2012 +0200

    crypto: testmgr - update LZO compression test vectors
    
    commit 0ec7382036922be063b515b2a3f1d6f7a607392c upstream.
    
    Update the LZO compression test vectors according to the latest compressor
    version.
    
    Signed-off-by: Markus F.X.J. Oberhumer <markus@oberhumer.com>
    Cc: Derrick Pallas <pallas@meraki.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
