commit 96427a5164f3eff6ee6eb924dbb53adb6eaa082d
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sun Mar 11 16:23:23 2018 +0100

    Linux 4.14.26

commit dc6fb79de47daa1cb38054af318ff6725eb473cc
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 1 22:16:21 2018 +0100

    KVM: x86: fix backward migration with async_PF
    
    commit fe2a3027e74e40a3ece3a4c1e4e51403090a907a upstream.
    
    Guests on new hypersiors might set KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT
    bit when enabling async_PF, but this bit is reserved on old hypervisors,
    which results in a failure upon migration.
    
    To avoid breaking different cases, we are checking for CPUID feature bit
    before enabling the feature and nothing else.
    
    Fixes: 52a5c155cf79 ("KVM: async_pf: Let guest support delivery of async_pf from guest mode")
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Wanpeng Li <wanpengli@tencent.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [jwang: port to 4.14]
    Signed-off-by: Jack Wang <jinpu.wang@profitbricks.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a91064ff43a26357c91be2c77f420a78dce6a19a
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:14:47 2018 +0100

    bpf, ppc64: fix out of bounds access in tail call
    
    [ upstream commit d269176e766c71c998cb75b4ea8cbc321cc0019d ]
    
    While working on 16338a9b3ac3 ("bpf, arm64: fix out of bounds access in
    tail call") I noticed that ppc64 JIT is partially affected as well. While
    the bound checking is correctly performed as unsigned comparison, the
    register with the index value however, is never truncated into 32 bit
    space, so e.g. a index value of 0x100000000ULL with a map of 1 element
    would pass with PPC_CMPLW() whereas we later on continue with the full
    64 bit register value. Therefore, as we do in interpreter and other JITs
    truncate the value to 32 bit initially in order to fix access.
    
    Fixes: ce0761419fae ("powerpc/bpf: Implement support for tail calls")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Tested-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3e272a8cd57abd9477a82b78220db1cc29d42270
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:14:46 2018 +0100

    bpf: allow xadd only on aligned memory
    
    [ upstream commit ca36960211eb228bcbc7aaebfa0d027368a94c60 ]
    
    The requirements around atomic_add() / atomic64_add() resp. their
    JIT implementations differ across architectures. E.g. while x86_64
    seems just fine with BPF's xadd on unaligned memory, on arm64 it
    triggers via interpreter but also JIT the following crash:
    
      [  830.864985] Unable to handle kernel paging request at virtual address ffff8097d7ed6703
      [...]
      [  830.916161] Internal error: Oops: 96000021 [#1] SMP
      [  830.984755] CPU: 37 PID: 2788 Comm: test_verifier Not tainted 4.16.0-rc2+ #8
      [  830.991790] Hardware name: Huawei TaiShan 2280 /BC11SPCD, BIOS 1.29 07/17/2017
      [  830.998998] pstate: 80400005 (Nzcv daif +PAN -UAO)
      [  831.003793] pc : __ll_sc_atomic_add+0x4/0x18
      [  831.008055] lr : ___bpf_prog_run+0x1198/0x1588
      [  831.012485] sp : ffff00001ccabc20
      [  831.015786] x29: ffff00001ccabc20 x28: ffff8017d56a0f00
      [  831.021087] x27: 0000000000000001 x26: 0000000000000000
      [  831.026387] x25: 000000c168d9db98 x24: 0000000000000000
      [  831.031686] x23: ffff000008203878 x22: ffff000009488000
      [  831.036986] x21: ffff000008b14e28 x20: ffff00001ccabcb0
      [  831.042286] x19: ffff0000097b5080 x18: 0000000000000a03
      [  831.047585] x17: 0000000000000000 x16: 0000000000000000
      [  831.052885] x15: 0000ffffaeca8000 x14: 0000000000000000
      [  831.058184] x13: 0000000000000000 x12: 0000000000000000
      [  831.063484] x11: 0000000000000001 x10: 0000000000000000
      [  831.068783] x9 : 0000000000000000 x8 : 0000000000000000
      [  831.074083] x7 : 0000000000000000 x6 : 000580d428000000
      [  831.079383] x5 : 0000000000000018 x4 : 0000000000000000
      [  831.084682] x3 : ffff00001ccabcb0 x2 : 0000000000000001
      [  831.089982] x1 : ffff8097d7ed6703 x0 : 0000000000000001
      [  831.095282] Process test_verifier (pid: 2788, stack limit = 0x0000000018370044)
      [  831.102577] Call trace:
      [  831.105012]  __ll_sc_atomic_add+0x4/0x18
      [  831.108923]  __bpf_prog_run32+0x4c/0x70
      [  831.112748]  bpf_test_run+0x78/0xf8
      [  831.116224]  bpf_prog_test_run_xdp+0xb4/0x120
      [  831.120567]  SyS_bpf+0x77c/0x1110
      [  831.123873]  el0_svc_naked+0x30/0x34
      [  831.127437] Code: 97fffe97 17ffffec 00000000 f9800031 (885f7c31)
    
    Reason for this is because memory is required to be aligned. In
    case of BPF, we always enforce alignment in terms of stack access,
    but not when accessing map values or packet data when the underlying
    arch (e.g. arm64) has CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS set.
    
    xadd on packet data that is local to us anyway is just wrong, so
    forbid this case entirely. The only place where xadd makes sense in
    fact are map values; xadd on stack is wrong as well, but it's been
    around for much longer. Specifically enforce strict alignment in case
    of xadd, so that we handle this case generically and avoid such crashes
    in the first place.
    
    Fixes: 17a5267067f3 ("bpf: verifier (add verifier core)")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e1760b3563fb31877a6d625f53ac69f1f3074633
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 8 13:14:45 2018 +0100

    bpf: add schedule points in percpu arrays management
    
    [ upstream commit 32fff239de37ef226d5b66329dd133f64d63b22d ]
    
    syszbot managed to trigger RCU detected stalls in
    bpf_array_free_percpu()
    
    It takes time to allocate a huge percpu map, but even more time to free
    it.
    
    Since we run in process context, use cond_resched() to yield cpu if
    needed.
    
    Fixes: a10423b87a7e ("bpf: introduce BPF_MAP_TYPE_PERCPU_ARRAY map")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 03549a3476e1c9b8b9a62105566f97743ffda0c0
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:14:44 2018 +0100

    bpf, arm64: fix out of bounds access in tail call
    
    [ upstream commit 16338a9b3ac30740d49f5dfed81bac0ffa53b9c7 ]
    
    I recently noticed a crash on arm64 when feeding a bogus index
    into BPF tail call helper. The crash would not occur when the
    interpreter is used, but only in case of JIT. Output looks as
    follows:
    
      [  347.007486] Unable to handle kernel paging request at virtual address fffb850e96492510
      [...]
      [  347.043065] [fffb850e96492510] address between user and kernel address ranges
      [  347.050205] Internal error: Oops: 96000004 [#1] SMP
      [...]
      [  347.190829] x13: 0000000000000000 x12: 0000000000000000
      [  347.196128] x11: fffc047ebe782800 x10: ffff808fd7d0fd10
      [  347.201427] x9 : 0000000000000000 x8 : 0000000000000000
      [  347.206726] x7 : 0000000000000000 x6 : 001c991738000000
      [  347.212025] x5 : 0000000000000018 x4 : 000000000000ba5a
      [  347.217325] x3 : 00000000000329c4 x2 : ffff808fd7cf0500
      [  347.222625] x1 : ffff808fd7d0fc00 x0 : ffff808fd7cf0500
      [  347.227926] Process test_verifier (pid: 4548, stack limit = 0x000000007467fa61)
      [  347.235221] Call trace:
      [  347.237656]  0xffff000002f3a4fc
      [  347.240784]  bpf_test_run+0x78/0xf8
      [  347.244260]  bpf_prog_test_run_skb+0x148/0x230
      [  347.248694]  SyS_bpf+0x77c/0x1110
      [  347.251999]  el0_svc_naked+0x30/0x34
      [  347.255564] Code: 9100075a d280220a 8b0a002a d37df04b (f86b694b)
      [...]
    
    In this case the index used in BPF r3 is the same as in r1
    at the time of the call, meaning we fed a pointer as index;
    here, it had the value 0xffff808fd7cf0500 which sits in x2.
    
    While I found tail calls to be working in general (also for
    hitting the error cases), I noticed the following in the code
    emission:
    
      # bpftool p d j i 988
      [...]
      38:   ldr     w10, [x1,x10]
      3c:   cmp     w2, w10
      40:   b.ge    0x000000000000007c              <-- signed cmp
      44:   mov     x10, #0x20                      // #32
      48:   cmp     x26, x10
      4c:   b.gt    0x000000000000007c
      50:   add     x26, x26, #0x1
      54:   mov     x10, #0x110                     // #272
      58:   add     x10, x1, x10
      5c:   lsl     x11, x2, #3
      60:   ldr     x11, [x10,x11]                  <-- faulting insn (f86b694b)
      64:   cbz     x11, 0x000000000000007c
      [...]
    
    Meaning, the tests passed because commit ddb55992b04d ("arm64:
    bpf: implement bpf_tail_call() helper") was using signed compares
    instead of unsigned which as a result had the test wrongly passing.
    
    Change this but also the tail call count test both into unsigned
    and cap the index as u32. Latter we did as well in 90caccdd8cc0
    ("bpf: fix bpf_tail_call() x64 JIT") and is needed in addition here,
    too. Tested on HiSilicon Hi1616.
    
    Result after patch:
    
      # bpftool p d j i 268
      [...]
      38:   ldr     w10, [x1,x10]
      3c:   add     w2, w2, #0x0
      40:   cmp     w2, w10
      44:   b.cs    0x0000000000000080
      48:   mov     x10, #0x20                      // #32
      4c:   cmp     x26, x10
      50:   b.hi    0x0000000000000080
      54:   add     x26, x26, #0x1
      58:   mov     x10, #0x110                     // #272
      5c:   add     x10, x1, x10
      60:   lsl     x11, x2, #3
      64:   ldr     x11, [x10,x11]
      68:   cbz     x11, 0x0000000000000080
      [...]
    
    Fixes: ddb55992b04d ("arm64: bpf: implement bpf_tail_call() helper")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7e657aa3b4f761f66239c73dbf67fae6a6249f12
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:14:43 2018 +0100

    bpf, x64: implement retpoline for tail call
    
    [ upstream commit a493a87f38cfa48caaa95c9347be2d914c6fdf29 ]
    
    Implement a retpoline [0] for the BPF tail call JIT'ing that converts
    the indirect jump via jmp %rax that is used to make the long jump into
    another JITed BPF image. Since this is subject to speculative execution,
    we need to control the transient instruction sequence here as well
    when CONFIG_RETPOLINE is set, and direct it into a pause + lfence loop.
    The latter aligns also with what gcc / clang emits (e.g. [1]).
    
    JIT dump after patch:
    
      # bpftool p d x i 1
       0: (18) r2 = map[id:1]
       2: (b7) r3 = 0
       3: (85) call bpf_tail_call#12
       4: (b7) r0 = 2
       5: (95) exit
    
    With CONFIG_RETPOLINE:
    
      # bpftool p d j i 1
      [...]
      33:   cmp    %edx,0x24(%rsi)
      36:   jbe    0x0000000000000072  |*
      38:   mov    0x24(%rbp),%eax
      3e:   cmp    $0x20,%eax
      41:   ja     0x0000000000000072  |
      43:   add    $0x1,%eax
      46:   mov    %eax,0x24(%rbp)
      4c:   mov    0x90(%rsi,%rdx,8),%rax
      54:   test   %rax,%rax
      57:   je     0x0000000000000072  |
      59:   mov    0x28(%rax),%rax
      5d:   add    $0x25,%rax
      61:   callq  0x000000000000006d  |+
      66:   pause                      |
      68:   lfence                     |
      6b:   jmp    0x0000000000000066  |
      6d:   mov    %rax,(%rsp)         |
      71:   retq                       |
      72:   mov    $0x2,%eax
      [...]
    
      * relative fall-through jumps in error case
      + retpoline for indirect jump
    
    Without CONFIG_RETPOLINE:
    
      # bpftool p d j i 1
      [...]
      33:   cmp    %edx,0x24(%rsi)
      36:   jbe    0x0000000000000063  |*
      38:   mov    0x24(%rbp),%eax
      3e:   cmp    $0x20,%eax
      41:   ja     0x0000000000000063  |
      43:   add    $0x1,%eax
      46:   mov    %eax,0x24(%rbp)
      4c:   mov    0x90(%rsi,%rdx,8),%rax
      54:   test   %rax,%rax
      57:   je     0x0000000000000063  |
      59:   mov    0x28(%rax),%rax
      5d:   add    $0x25,%rax
      61:   jmpq   *%rax               |-
      63:   mov    $0x2,%eax
      [...]
    
      * relative fall-through jumps in error case
      - plain indirect jump as before
    
      [0] https://support.google.com/faqs/answer/7625886
      [1] https://github.com/gcc-mirror/gcc/commit/a31e654fa107be968b802786d747e962c2fcdb2b
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 853223c2caf47a46f2c828ec7f2e1731f01e93b7
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Mar 8 13:14:42 2018 +0100

    bpf: fix rcu lockdep warning for lpm_trie map_free callback
    
    [ upstream commit 6c5f61023c5b0edb0c8a64c902fe97c6453b1852 ]
    
    Commit 9a3efb6b661f ("bpf: fix memory leak in lpm_trie map_free callback function")
    fixed a memory leak and removed unnecessary locks in map_free callback function.
    Unfortrunately, it introduced a lockdep warning. When lockdep checking is turned on,
    running tools/testing/selftests/bpf/test_lpm_map will have:
    
      [   98.294321] =============================
      [   98.294807] WARNING: suspicious RCU usage
      [   98.295359] 4.16.0-rc2+ #193 Not tainted
      [   98.295907] -----------------------------
      [   98.296486] /home/yhs/work/bpf/kernel/bpf/lpm_trie.c:572 suspicious rcu_dereference_check() usage!
      [   98.297657]
      [   98.297657] other info that might help us debug this:
      [   98.297657]
      [   98.298663]
      [   98.298663] rcu_scheduler_active = 2, debug_locks = 1
      [   98.299536] 2 locks held by kworker/2:1/54:
      [   98.300152]  #0:  ((wq_completion)"events"){+.+.}, at: [<00000000196bc1f0>] process_one_work+0x157/0x5c0
      [   98.301381]  #1:  ((work_completion)(&map->work)){+.+.}, at: [<00000000196bc1f0>] process_one_work+0x157/0x5c0
    
    Since actual trie tree removal happens only after no other
    accesses to the tree are possible, replacing
      rcu_dereference_protected(*slot, lockdep_is_held(&trie->lock))
    with
      rcu_dereference_protected(*slot, 1)
    fixed the issue.
    
    Fixes: 9a3efb6b661f ("bpf: fix memory leak in lpm_trie map_free callback function")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 62a2caa5027fc0fe0d6a203ef8bb20a2033b80cb
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Mar 8 13:14:41 2018 +0100

    bpf: fix memory leak in lpm_trie map_free callback function
    
    [ upstream commit 9a3efb6b661f71d5675369ace9257833f0e78ef3 ]
    
    There is a memory leak happening in lpm_trie map_free callback
    function trie_free. The trie structure itself does not get freed.
    
    Also, trie_free function did not do synchronize_rcu before freeing
    various data structures. This is incorrect as some rcu_read_lock
    region(s) for lookup, update, delete or get_next_key may not complete yet.
    The fix is to add synchronize_rcu in the beginning of trie_free.
    The useless spin_lock is removed from this function as well.
    
    Fixes: b95a5c4db09b ("bpf: add a longest prefix match trie map implementation")
    Reported-by: Mathieu Malaterre <malat@debian.org>
    Reported-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d9fd73c60bc90e417274d4900d810479c7632477
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:14:40 2018 +0100

    bpf: fix mlock precharge on arraymaps
    
    [ upstream commit 9c2d63b843a5c8a8d0559cc067b5398aa5ec3ffc ]
    
    syzkaller recently triggered OOM during percpu map allocation;
    while there is work in progress by Dennis Zhou to add __GFP_NORETRY
    semantics for percpu allocator under pressure, there seems also a
    missing bpf_map_precharge_memlock() check in array map allocation.
    
    Given today the actual bpf_map_charge_memlock() happens after the
    find_and_alloc_map() in syscall path, the bpf_map_precharge_memlock()
    is there to bail out early before we go and do the map setup work
    when we find that we hit the limits anyway. Therefore add this for
    array map as well.
    
    Fixes: 6c9059817432 ("bpf: pre-allocate hash map elements")
    Fixes: a10423b87a7e ("bpf: introduce BPF_MAP_TYPE_PERCPU_ARRAY map")
    Reported-by: syzbot+adb03f3f0bb57ce3acda@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Dennis Zhou <dennisszhou@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
