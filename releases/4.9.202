commit a86e4a77b558b9bdbae1192188ea744a3cf84176
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sat Nov 16 10:29:58 2019 +0100

    Linux 4.9.202

commit ca60c77067d4cde390e1f58a6f947c7c7fb75f97
Author: Gomez Iglesias, Antonio <antonio.gomez.iglesias@intel.com>
Date:   Mon Nov 4 12:22:03 2019 +0100

    Documentation: Add ITLB_MULTIHIT documentation
    
    commit 7f00cc8d4a51074eb0ad4c3f16c15757b1ddfb7d upstream.
    
    Add the initial ITLB_MULTIHIT documentation.
    
    [ tglx: Add it to the index so it gets actually built. ]
    
    Signed-off-by: Antonio Gomez Iglesias <antonio.gomez.iglesias@intel.com>
    Signed-off-by: Nelson D'Souza <nelson.dsouza@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [bwh: Backported to 4.9: adjust filenames]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c6e94acbf6abab3e3c25fcdd3343d0c2a3f160ca
Author: Junaid Shahid <junaids@google.com>
Date:   Mon Nov 4 12:22:03 2019 +0100

    kvm: x86: mmu: Recovery of shattered NX large pages
    
    commit 1aa9b9572b10529c2e64e2b8f44025d86e124308 upstream.
    
    The page table pages corresponding to broken down large pages are zapped in
    FIFO order, so that the large page can potentially be recovered, if it is
    not longer being used for execution.  This removes the performance penalty
    for walking deeper EPT page tables.
    
    By default, one large page will last about one hour once the guest
    reaches a steady state.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [bwh: Backported to 4.9:
     - Update another error path in kvm_create_vm() to use out_err_no_mmu_notifier
     - Adjust filename, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 61e191b467f1edea6fae9123c37355133273a31a
Author: Junaid Shahid <junaids@google.com>
Date:   Mon Nov 4 12:22:02 2019 +0100

    kvm: Add helper function for creating VM worker threads
    
    commit c57c80467f90e5504c8df9ad3555d2c78800bf94 upstream.
    
    Add a function to create a kernel thread associated with a given VM. In
    particular, it ensures that the worker thread inherits the priority and
    cgroups of the calling thread.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [bwh: Backported to 4.9: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a7ad7943b84fae87f5be18f05025c51ae103f732
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 4 12:22:02 2019 +0100

    kvm: mmu: ITLB_MULTIHIT mitigation
    
    commit b8e8c8303ff28c61046a4d0f6ea99aea609a7dc0 upstream.
    
    With some Intel processors, putting the same virtual address in the TLB
    as both a 4 KiB and 2 MiB page can confuse the instruction fetch unit
    and cause the processor to issue a machine check resulting in a CPU lockup.
    
    Unfortunately when EPT page tables use huge pages, it is possible for a
    malicious guest to cause this situation.
    
    Add a knob to mark huge pages as non-executable. When the nx_huge_pages
    parameter is enabled (and we are using EPT), all huge pages are marked as
    NX. If the guest attempts to execute in one of those pages, the page is
    broken down into 4K pages, which are then marked executable.
    
    This is not an issue for shadow paging (except nested EPT), because then
    the host is in control of TLB flushes and the problematic situation cannot
    happen.  With nested EPT, again the nested guest can cause problems shadow
    and direct EPT is treated in the same way.
    
    [ tglx: Fixup default to auto and massage wording a bit ]
    
    Originally-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [bwh: Backported to 4.9:
     - Use kvm_mmu_invalidate_zap_all_pages() instead of kvm_mmu_zap_all_fast()
     - Don't provide mode for nx_largepages_splitted as all stats are read-only
     - Adjust filename, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e2bd0778adc4b13e3874b48eaad689e4a3a35833
Author: Tyler Hicks <tyhicks@canonical.com>
Date:   Mon Nov 4 12:22:02 2019 +0100

    cpu/speculation: Uninline and export CPU mitigations helpers
    
    commit 731dc9df975a5da21237a18c3384f811a7a41cc6 upstream.
    
    A kernel module may need to check the value of the "mitigations=" kernel
    command line parameter as part of its setup when the module needs
    to perform software mitigations for a CPU flaw.
    
    Uninline and export the helper functions surrounding the cpu_mitigations
    enum to allow for their usage from a module.
    
    Lastly, privatize the enum and cpu_mitigations variable since the value of
    cpu_mitigations can be checked with the exported helper functions.
    
    Signed-off-by: Tyler Hicks <tyhicks@canonical.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 12ceedb7604dfbe370a21df444819ece665c91db
Author: Vineela Tummalapalli <vineela.tummalapalli@intel.com>
Date:   Mon Nov 4 12:22:01 2019 +0100

    x86/bugs: Add ITLB_MULTIHIT bug infrastructure
    
    commit db4d30fbb71b47e4ecb11c4efa5d8aad4b03dfae upstream.
    
    Some processors may incur a machine check error possibly resulting in an
    unrecoverable CPU lockup when an instruction fetch encounters a TLB
    multi-hit in the instruction TLB. This can occur when the page size is
    changed along with either the physical address or cache type. The relevant
    erratum can be found here:
    
       https://bugzilla.kernel.org/show_bug.cgi?id=205195
    
    There are other processors affected for which the erratum does not fully
    disclose the impact.
    
    This issue affects both bare-metal x86 page tables and EPT.
    
    It can be mitigated by either eliminating the use of large pages or by
    using careful TLB invalidations when changing the page size in the page
    tables.
    
    Just like Spectre, Meltdown, L1TF and MDS, a new bit has been allocated in
    MSR_IA32_ARCH_CAPABILITIES (PSCHANGE_MC_NO) and will be set on CPUs which
    are mitigated against this issue.
    
    Signed-off-by: Vineela Tummalapalli <vineela.tummalapalli@intel.com>
    Co-developed-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [bwh: Backported to 4.9:
     - No support for X86_VENDOR_HYGON, ATOM_AIRMONT_NP
     - Adjust context, indentation]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1b08d2ab698ddf78833717908e2a41336ab9c6ef
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun Oct 27 16:23:23 2019 +0100

    KVM: vmx, svm: always run with EFER.NXE=1 when shadow paging is active
    
    commit 9167ab79936206118cc60e47dcb926c3489f3bd5 upstream.
    
    VMX already does so if the host has SMEP, in order to support the combination of
    CR0.WP=1 and CR4.SMEP=1.  However, it is perfectly safe to always do so, and in
    fact VMX already ends up running with EFER.NXE=1 on old processors that lack the
    "load EFER" controls, because it may help avoiding a slow MSR write.  Removing
    all the conditionals simplifies the code.
    
    SVM does not have similar code, but it should since recent AMD processors do
    support SMEP.  So this patch also makes the code for the two vendors more similar
    while fixing NPT=0, CR0.WP=1 and CR4.SMEP=1 on AMD processors.
    
    Cc: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [bwh: Backported to 4.9: adjust filename]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 52644d80850a3fe965ee964e903acef7be61a62c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jul 1 06:22:57 2019 -0400

    KVM: x86: add tracepoints around __direct_map and FNAME(fetch)
    
    commit 335e192a3fa415e1202c8b9ecdaaecd643f823cc upstream.
    
    These are useful in debugging shadow paging.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [bwh: Backported to 4.9:
     - Keep using PT_PRESENT_MASK to test page write permission
     - Adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9dc6bc3f22f08099a27c38c68983fbc419e879f3
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sat Oct 19 18:10:55 2019 +0100

    KVM: x86: Add is_executable_pte()
    
    Extracted from commit d3e328f2cb01 "kvm: x86: mmu: Verify that
    restored PTE has needed perms in fast page fault".
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 515fa37a3e550ac7f291fb295b4e6174b6bbbd85
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun Jun 30 08:36:21 2019 -0400

    KVM: x86: change kvm_mmu_page_get_gfn BUG_ON to WARN_ON
    
    commit e9f2a760b158551bfbef6db31d2cae45ab8072e5 upstream.
    
    Note that in such a case it is quite likely that KVM will BUG_ON
    in __pte_list_remove when the VM is closed.  However, there is no
    immediate risk of memory corruption in the host so a WARN_ON is
    enough and it lets you gather traces for debugging.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1d48204bd77090c950b13106ab51821729ae0d72
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun Jun 23 19:15:49 2019 +0200

    KVM: x86: remove now unneeded hugepage gfn adjustment
    
    commit d679b32611c0102ce33b9e1a4e4b94854ed1812a upstream.
    
    After the previous patch, the low bits of the gfn are masked in
    both FNAME(fetch) and __direct_map, so we do not need to clear them
    in transparent_hugepage_adjust.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2e013f0fa714399d91c0fc9e3e4d64a9b517db6c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jun 24 13:06:21 2019 +0200

    KVM: x86: make FNAME(fetch) and __direct_map more similar
    
    commit 3fcf2d1bdeb6a513523cb2c77012a6b047aa859c upstream.
    
    These two functions are basically doing the same thing through
    kvm_mmu_get_page, link_shadow_page and mmu_set_spte; yet, for historical
    reasons, their code looks very different.  This patch tries to take the
    best of each and make them very similar, so that it is easy to understand
    changes that apply to both of them.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [bwh: Backported to 4.9: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2f57300f68fb40acff2e350686ec5e41463526c1
Author: Junaid Shahid <junaids@google.com>
Date:   Thu Jan 3 16:22:21 2019 -0800

    kvm: x86: Do not release the page inside mmu_set_spte()
    
    commit 43fdcda96e2550c6d1c46fb8a78801aa2f7276ed upstream.
    
    Release the page at the call-site where it was originally acquired.
    This makes the exit code cleaner for most call sites, since they
    do not need to duplicate code between success and the failure
    label.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c6170b81e7b78942cb4b36fc72cbd75145fd08d5
Author: Junaid Shahid <junaids@google.com>
Date:   Thu Jan 3 17:14:28 2019 -0800

    kvm: Convert kvm_lock to a mutex
    
    commit 0d9ce162cf46c99628cc5da9510b959c7976735b upstream.
    
    It doesn't seem as if there is any particular need for kvm_lock to be a
    spinlock, so convert the lock to a mutex so that sleepable functions (in
    particular cond_resched()) can be called while holding it.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [bwh: Backported to 4.9:
     - Drop changes in kvm_hyperv_tsc_notifier(), vm_stat_clear(),
       vcpu_stat_clear(), kvm_uevent_notify_change()
     - Adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 61524f1bccc041b7871a21984b69b8e538f446c0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Aug 17 15:03:32 2017 +0200

    KVM: x86: extend usage of RET_MMIO_PF_* constants
    
    commit 9b8ebbdb74b5ad76b9dfd8b101af17839174b126 upstream.
    
    The x86 MMU if full of code that returns 0 and 1 for retry/emulate.  Use
    the existing RET_MMIO_PF_RETRY/RET_MMIO_PF_EMULATE enum, renaming it to
    drop the MMIO part.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [bwh: Backported to 4.9: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 43a39a3e9b0573cd4383e52d8ded9965ae8994c5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Aug 17 18:36:56 2017 +0200

    KVM: x86: simplify ept_misconfig
    
    commit e08d26f0712532c79b5ba6200862eaf2036f8df6 upstream.
    
    Calling handle_mmio_page_fault() has been unnecessary since commit
    e9ee956e311d ("KVM: x86: MMU: Move handle_mmio_page_fault() call to
    kvm_mmu_page_fault()", 2016-02-22).
    
    handle_mmio_page_fault() can now be made static.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    [bwh: Backported to 4.9: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 042a4417d136559d7285ea6affdcbbd0e37192b6
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Nov 6 20:26:46 2019 -0600

    x86/speculation/taa: Fix printing of TAA_MSG_SMT on IBRS_ALL CPUs
    
    commit 012206a822a8b6ac09125bfaa210a95b9eb8f1c1 upstream.
    
    For new IBRS_ALL CPUs, the Enhanced IBRS check at the beginning of
    cpu_bugs_smt_update() causes the function to return early, unintentionally
    skipping the MDS and TAA logic.
    
    This is not a problem for MDS, because there appears to be no overlap
    between IBRS_ALL and MDS-affected CPUs.  So the MDS mitigation would be
    disabled and nothing would need to be done in this function anyway.
    
    But for TAA, the TAA_MSG_SMT string will never get printed on Cascade
    Lake and newer.
    
    The check is superfluous anyway: when 'spectre_v2_enabled' is
    SPECTRE_V2_IBRS_ENHANCED, 'spectre_v2_user' is always
    SPECTRE_V2_USER_NONE, and so the 'spectre_v2_user' switch statement
    handles it appropriately by doing nothing.  So just remove the check.
    
    Fixes: 1b42f017415b ("x86/speculation/taa: Add mitigation for TSX Async Abort")
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tyler Hicks <tyhicks@canonical.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0fbf080197189a30f75615dc2c45e2af561facd3
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Oct 23 12:35:50 2019 +0200

    x86/tsx: Add config options to set tsx=on|off|auto
    
    commit db616173d787395787ecc93eef075fa975227b10 upstream.
    
    There is a general consensus that TSX usage is not largely spread while
    the history shows there is a non trivial space for side channel attacks
    possible. Therefore the tsx is disabled by default even on platforms
    that might have a safe implementation of TSX according to the current
    knowledge. This is a fair trade off to make.
    
    There are, however, workloads that really do benefit from using TSX and
    updating to a newer kernel with TSX disabled might introduce a
    noticeable regressions. This would be especially a problem for Linux
    distributions which will provide TAA mitigations.
    
    Introduce config options X86_INTEL_TSX_MODE_OFF, X86_INTEL_TSX_MODE_ON
    and X86_INTEL_TSX_MODE_AUTO to control the TSX feature. The config
    setting can be overridden by the tsx cmdline options.
    
     [ bp: Text cleanups from Josh. ]
    
    Suggested-by: Borislav Petkov <bpetkov@suse.de>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [bwh: Backported to 4.9: adjust doc filename]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ba54aadc5c641dfe4e387edc977e07cd175ed59b
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 12:32:55 2019 +0200

    x86/speculation/taa: Add documentation for TSX Async Abort
    
    commit a7a248c593e4fd7a67c50b5f5318fe42a0db335e upstream.
    
    Add the documenation for TSX Async Abort. Include the description of
    the issue, how to check the mitigation state, control the mitigation,
    guidance for system administrators.
    
     [ bp: Add proper SPDX tags, touch ups by Josh and me. ]
    
    Co-developed-by: Antonio Gomez Iglesias <antonio.gomez.iglesias@intel.com>
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Antonio Gomez Iglesias <antonio.gomez.iglesias@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Mark Gross <mgross@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [bwh: Backported to 4.9: adjust filenames, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 562afad430aaf280d224c65589d8db4e29ca8ace
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 12:28:57 2019 +0200

    x86/tsx: Add "auto" option to the tsx= cmdline parameter
    
    commit 7531a3596e3272d1f6841e0d601a614555dc6b65 upstream.
    
    Platforms which are not affected by X86_BUG_TAA may want the TSX feature
    enabled. Add "auto" option to the TSX cmdline parameter. When tsx=auto
    disable TSX when X86_BUG_TAA is present, otherwise enable TSX.
    
    More details on X86_BUG_TAA can be found here:
    https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html
    
     [ bp: Extend the arg buffer to accommodate "auto\0". ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [bwh: Backported to 4.9: adjust filename]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 639453597dcce3337990c80272fae6b8e2c93005
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 12:23:33 2019 +0200

    kvm/x86: Export MDS_NO=0 to guests when TSX is enabled
    
    commit e1d38b63acd843cfdd4222bf19a26700fd5c699e upstream.
    
    Export the IA32_ARCH_CAPABILITIES MSR bit MDS_NO=0 to guests on TSX
    Async Abort(TAA) affected hosts that have TSX enabled and updated
    microcode. This is required so that the guests don't complain,
    
      "Vulnerable: Clear CPU buffers attempted, no microcode"
    
    when the host has the updated microcode to clear CPU buffers.
    
    Microcode update also adds support for MSR_IA32_TSX_CTRL which is
    enumerated by the ARCH_CAP_TSX_CTRL bit in IA32_ARCH_CAPABILITIES MSR.
    Guests can't do this check themselves when the ARCH_CAP_TSX_CTRL bit is
    not exported to the guests.
    
    In this case export MDS_NO=0 to the guests. When guests have
    CPUID.MD_CLEAR=1, they deploy MDS mitigation which also mitigates TAA.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Neelima Krishnan <neelima.krishnan@intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9392b2dda0aedff871f10eae4e9b1e7d7e7bc3f9
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 12:19:51 2019 +0200

    x86/speculation/taa: Add sysfs reporting for TSX Async Abort
    
    commit 6608b45ac5ecb56f9e171252229c39580cc85f0f upstream.
    
    Add the sysfs reporting file for TSX Async Abort. It exposes the
    vulnerability and the mitigation state similar to the existing files for
    the other hardware vulnerabilities.
    
    Sysfs file path is:
    /sys/devices/system/cpu/vulnerabilities/tsx_async_abort
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Neelima Krishnan <neelima.krishnan@intel.com>
    Reviewed-by: Mark Gross <mgross@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a117aa4e6876fa4b272d2f0b5f12232a04cce895
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 11:30:45 2019 +0200

    x86/speculation/taa: Add mitigation for TSX Async Abort
    
    commit 1b42f017415b46c317e71d41c34ec088417a1883 upstream.
    
    TSX Async Abort (TAA) is a side channel vulnerability to the internal
    buffers in some Intel processors similar to Microachitectural Data
    Sampling (MDS). In this case, certain loads may speculatively pass
    invalid data to dependent operations when an asynchronous abort
    condition is pending in a TSX transaction.
    
    This includes loads with no fault or assist condition. Such loads may
    speculatively expose stale data from the uarch data structures as in
    MDS. Scope of exposure is within the same-thread and cross-thread. This
    issue affects all current processors that support TSX, but do not have
    ARCH_CAP_TAA_NO (bit 8) set in MSR_IA32_ARCH_CAPABILITIES.
    
    On CPUs which have their IA32_ARCH_CAPABILITIES MSR bit MDS_NO=0,
    CPUID.MD_CLEAR=1 and the MDS mitigation is clearing the CPU buffers
    using VERW or L1D_FLUSH, there is no additional mitigation needed for
    TAA. On affected CPUs with MDS_NO=1 this issue can be mitigated by
    disabling the Transactional Synchronization Extensions (TSX) feature.
    
    A new MSR IA32_TSX_CTRL in future and current processors after a
    microcode update can be used to control the TSX feature. There are two
    bits in that MSR:
    
    * TSX_CTRL_RTM_DISABLE disables the TSX sub-feature Restricted
    Transactional Memory (RTM).
    
    * TSX_CTRL_CPUID_CLEAR clears the RTM enumeration in CPUID. The other
    TSX sub-feature, Hardware Lock Elision (HLE), is unconditionally
    disabled with updated microcode but still enumerated as present by
    CPUID(EAX=7).EBX{bit4}.
    
    The second mitigation approach is similar to MDS which is clearing the
    affected CPU buffers on return to user space and when entering a guest.
    Relevant microcode update is required for the mitigation to work.  More
    details on this approach can be found here:
    
      https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html
    
    The TSX feature can be controlled by the "tsx" command line parameter.
    If it is force-enabled then "Clear CPU buffers" (MDS mitigation) is
    deployed. The effective mitigation state can be read from sysfs.
    
     [ bp:
       - massage + comments cleanup
       - s/TAA_MITIGATION_TSX_DISABLE/TAA_MITIGATION_TSX_DISABLED/g - Josh.
       - remove partial TAA mitigation in update_mds_branch_idle() - Josh.
       - s/tsx_async_abort_cmdline/tsx_async_abort_parse_cmdline/g
     ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [bwh: Backported to 4.9:
     - Add #include "cpu.h" in bugs.c
     - Adjust context, indentation]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 211278805ea59ef5b871d89f5688e50faf6ca68c
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 11:01:53 2019 +0200

    x86/cpu: Add a "tsx=" cmdline option with TSX disabled by default
    
    commit 95c5824f75f3ba4c9e8e5a4b1a623c95390ac266 upstream.
    
    Add a kernel cmdline parameter "tsx" to control the Transactional
    Synchronization Extensions (TSX) feature. On CPUs that support TSX
    control, use "tsx=on|off" to enable or disable TSX. Not specifying this
    option is equivalent to "tsx=off". This is because on certain processors
    TSX may be used as a part of a speculative side channel attack.
    
    Carve out the TSX controlling functionality into a separate compilation
    unit because TSX is a CPU feature while the TSX async abort control
    machinery will go to cpu/bugs.c.
    
     [ bp: - Massage, shorten and clear the arg buffer.
           - Clarifications of the tsx= possible options - Josh.
           - Expand on TSX_CTRL availability - Pawan. ]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [bwh: Backported to 4.9: adjust filenames, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 919d56194a7fe18c8d67e873d6f71c9db2e00eea
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 10:52:35 2019 +0200

    x86/cpu: Add a helper function x86_read_arch_cap_msr()
    
    commit 286836a70433fb64131d2590f4bf512097c255e1 upstream.
    
    Add a helper function to read the IA32_ARCH_CAPABILITIES MSR.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Neelima Krishnan <neelima.krishnan@intel.com>
    Reviewed-by: Mark Gross <mgross@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2fc508384968d5796e005bf85d2daf2f16510119
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Oct 23 10:45:50 2019 +0200

    x86/msr: Add the IA32_TSX_CTRL MSR
    
    commit c2955f270a84762343000f103e0640d29c7a96f3 upstream.
    
    Transactional Synchronization Extensions (TSX) may be used on certain
    processors as part of a speculative side channel attack.  A microcode
    update for existing processors that are vulnerable to this attack will
    add a new MSR - IA32_TSX_CTRL to allow the system administrator the
    option to disable TSX as one of the possible mitigations.
    
    The CPUs which get this new MSR after a microcode upgrade are the ones
    which do not set MSR_IA32_ARCH_CAPABILITIES.MDS_NO (bit 5) because those
    CPUs have CPUID.MD_CLEAR, i.e., the VERW implementation which clears all
    CPU buffers takes care of the TAA case as well.
    
      [ Note that future processors that are not vulnerable will also
        support the IA32_TSX_CTRL MSR. ]
    
    Add defines for the new IA32_TSX_CTRL MSR and its bits.
    
    TSX has two sub-features:
    
    1. Restricted Transactional Memory (RTM) is an explicitly-used feature
       where new instructions begin and end TSX transactions.
    2. Hardware Lock Elision (HLE) is implicitly used when certain kinds of
       "old" style locks are used by software.
    
    Bit 7 of the IA32_ARCH_CAPABILITIES indicates the presence of the
    IA32_TSX_CTRL MSR.
    
    There are two control bits in IA32_TSX_CTRL MSR:
    
      Bit 0: When set, it disables the Restricted Transactional Memory (RTM)
             sub-feature of TSX (will force all transactions to abort on the
             XBEGIN instruction).
    
      Bit 1: When set, it disables the enumeration of the RTM and HLE feature
             (i.e. it will make CPUID(EAX=7).EBX{bit4} and
              CPUID(EAX=7).EBX{bit11} read as 0).
    
    The other TSX sub-feature, Hardware Lock Elision (HLE), is
    unconditionally disabled by the new microcode but still enumerated
    as present by CPUID(EAX=7).EBX{bit4}, unless disabled by
    IA32_TSX_CTRL_MSR[1] - TSX_CTRL_CPUID_CLEAR.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Neelima Krishnan <neelima.krishnan@intel.com>
    Reviewed-by: Mark Gross <mgross@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e83ef92e99792e3ec88b95839e57c300ef692900
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Aug 19 17:24:07 2019 +0200

    KVM: x86: use Intel speculation bugs and features as derived in generic x86 code
    
    commit 0c54914d0c52a15db9954a76ce80fee32cf318f4 upstream.
    
    Similar to AMD bits, set the Intel bits from the vendor-independent
    feature and bug flags, because KVM_GET_SUPPORTED_CPUID does not care
    about the vendor and they should be set on AMD processors as well.
    
    Suggested-by: Jim Mattson <jmattson@google.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6e3683ebef352a0581805d499800be465ca84bc5
Author: Jack Pham <jackp@codeaurora.org>
Date:   Tue Aug 1 02:00:56 2017 -0700

    usb: gadget: core: unmap request from DMA only if previously mapped
    
    commit 31fe084ffaaf8abece14f8ca28e5e3b4e2bf97b6 upstream.
    
    In the SG case this is already handled since a non-zero
    request->num_mapped_sgs is a clear indicator that dma_map_sg()
    had been called. While it would be nice to do the same for the
    singly mapped case by simply checking for non-zero request->dma,
    it's conceivable that 0 is a valid dma_addr_t handle. Hence add
    a flag 'dma_mapped' to struct usb_request and use this to
    determine the need to call dma_unmap_single(). Otherwise, if a
    request is not DMA mapped then the result of calling
    usb_request_unmap_request() would safely be a no-op.
    
    Signed-off-by: Jack Pham <jackp@codeaurora.org>
    Signed-off-by: Felipe Balbi <felipe.balbi@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e9e0278781e5ec5b94274eceb4940a30505cb390
Author: Jonas Gorski <jonas.gorski@gmail.com>
Date:   Mon Dec 10 12:40:38 2018 +0100

    MIPS: BCM63XX: fix switch core reset on BCM6368
    
    commit 8a38dacf87180738d42b058334c951eba15d2d47 upstream.
    
    The Ethernet Switch core mask was set to 0, causing the switch core to
    be not reset on BCM6368 on boot. Provide the proper mask so the switch
    core gets reset to a known good state.
    
    Fixes: 799faa626c71 ("MIPS: BCM63XX: add core reset helper")
    Signed-off-by: Jonas Gorski <jonas.gorski@gmail.com>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3858f013de0ae5a19b8276944e12fd01b0cac979
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Sat Feb 23 12:33:27 2019 +0800

    Bluetooth: hci_ldisc: Postpone HCI_UART_PROTO_READY bit set in hci_uart_set_proto()
    
    commit 56897b217a1d0a91c9920cb418d6b3fe922f590a upstream.
    
    task A:                                task B:
    hci_uart_set_proto                     flush_to_ldisc
     - p->open(hu) -> h5_open  //alloc h5  - receive_buf
     - set_bit HCI_UART_PROTO_READY         - tty_port_default_receive_buf
     - hci_uart_register_dev                 - tty_ldisc_receive_buf
                                              - hci_uart_tty_receive
                                               - test_bit HCI_UART_PROTO_READY
                                                - h5_recv
     - clear_bit HCI_UART_PROTO_READY             while() {
     - p->open(hu) -> h5_close //free h5
                                                  - h5_rx_3wire_hdr
                                                   - h5_reset()  //use-after-free
                                                  }
    
    It could use ioctl to set hci uart proto, but there is
    a use-after-free issue when hci_uart_register_dev() fail in
    hci_uart_set_proto(), see stack above, fix this by setting
    HCI_UART_PROTO_READY bit only when hci_uart_register_dev()
    return success.
    
    Reported-by: syzbot+899a33dc0fa0dbaf06a6@syzkaller.appspotmail.com
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Reviewed-by: Jeremy Cline <jcline@redhat.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7b93d92338a736b87730e2421558d7b5b7d0d5ac
Author: Junaid Shahid <junaids@google.com>
Date:   Mon Nov 11 15:50:19 2019 -0800

    kvm: mmu: Don't read PDPTEs when paging is not enabled
    
    [ Upstream commit d35b34a9a70edae7ef923f100e51b8b5ae9fe899 ]
    
    kvm should not attempt to read guest PDPTEs when CR0.PG = 0 and
    CR4.PAE = 1.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
