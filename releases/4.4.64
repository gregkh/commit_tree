commit 12f4e1f54a1334bcd5f9586fc9eb7baefb14b826
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Thu Apr 27 09:09:53 2017 +0200

    Linux 4.4.64

commit 6862fa9077dea0ec2ba5e6ea7c7f90b786288596
Author: Jon Paul Maloy <jon.maloy@ericsson.com>
Date:   Wed Feb 24 11:10:48 2016 -0500

    tipc: fix crash during node removal
    
    commit d25a01257e422a4bdeb426f69529d57c73b235fe upstream.
    
    When the TIPC module is unloaded, we have identified a race condition
    that allows a node reference counter to go to zero and the node instance
    being freed before the node timer is finished with accessing it. This
    leads to occasional crashes, especially in multi-namespace environments.
    
    The scenario goes as follows:
    
    CPU0:(node_stop)                       CPU1:(node_timeout)  // ref == 2
    
    1:                                          if(!mod_timer())
    2: if (del_timer())
    3:   tipc_node_put()                                        // ref -> 1
    4: tipc_node_put()                                          // ref -> 0
    5:   kfree_rcu(node);
    6:                                               tipc_node_get(node)
    7:                                               // BOOM!
    
    We now clean up this functionality as follows:
    
    1) We remove the node pointer from the node lookup table before we
       attempt deactivating the timer. This way, we reduce the risk that
       tipc_node_find() may obtain a valid pointer to an instance marked
       for deletion; a harmless but undesirable situation.
    
    2) We use del_timer_sync() instead of del_timer() to safely deactivate
       the node timer without any risk that it might be reactivated by the
       timeout handler. There is no risk of deadlock here, since the two
       functions never touch the same spinlocks.
    
    3: We remove a pointless tipc_node_get() + tipc_node_put() from the
       timeout handler.
    
    Reported-by: Zhijiang Hu <huzhijiang@gmail.com>
    Acked-by: Ying Xue <ying.xue@windriver.com>
    Signed-off-by: Jon Maloy <jon.maloy@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6ddbac9aa800b99761a05ba520d62a0d8063a5b8
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Dec 29 14:02:29 2015 -0800

    block: fix del_gendisk() vs blkdev_ioctl crash
    
    commit ac34f15e0c6d2fd58480052b6985f6991fb53bcc upstream.
    
    When tearing down a block device early in its lifetime, userspace may
    still be performing discovery actions like blkdev_ioctl() to re-read
    partitions.
    
    The nvdimm_revalidate_disk() implementation depends on
    disk->driverfs_dev to be valid at entry.  However, it is set to NULL in
    del_gendisk() and fatally this is happening *before* the disk device is
    deleted from userspace view.
    
    There's no reason for del_gendisk() to clear ->driverfs_dev.  That
    device is the parent of the disk.  It is guaranteed to not be freed
    until the disk, as a child, drops its ->parent reference.
    
    We could also fix this issue locally in nvdimm_revalidate_disk() by
    using disk_to_dev(disk)->parent, but lets fix it globally since
    ->driverfs_dev follows the lifetime of the parent.  Longer term we
    should probably just add a @parent parameter to add_disk(), and stop
    carrying this pointer in the gendisk.
    
     BUG: unable to handle kernel NULL pointer dereference at           (null)
     IP: [<ffffffffa00340a8>] nvdimm_revalidate_disk+0x18/0x90 [libnvdimm]
     CPU: 2 PID: 538 Comm: systemd-udevd Tainted: G           O    4.4.0-rc5 #2257
     [..]
     Call Trace:
      [<ffffffff8143e5c7>] rescan_partitions+0x87/0x2c0
      [<ffffffff810f37f9>] ? __lock_is_held+0x49/0x70
      [<ffffffff81438c62>] __blkdev_reread_part+0x72/0xb0
      [<ffffffff81438cc5>] blkdev_reread_part+0x25/0x40
      [<ffffffff8143982d>] blkdev_ioctl+0x4fd/0x9c0
      [<ffffffff811246c9>] ? current_kernel_time64+0x69/0xd0
      [<ffffffff812916dd>] block_ioctl+0x3d/0x50
      [<ffffffff81264c38>] do_vfs_ioctl+0x308/0x560
      [<ffffffff8115dbd1>] ? __audit_syscall_entry+0xb1/0x100
      [<ffffffff810031d6>] ? do_audit_syscall_entry+0x66/0x70
      [<ffffffff81264f09>] SyS_ioctl+0x79/0x90
      [<ffffffff81902672>] entry_SYSCALL_64_fastpath+0x12/0x76
    
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@fb.com>
    Reported-by: Robert Hu <robert.hu@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d1cc3cdd39e90e70ee5fc9a766ad3a6c61426fa8
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Apr 6 09:04:31 2017 -0700

    x86, pmem: fix broken __copy_user_nocache cache-bypass assumptions
    
    commit 11e63f6d920d6f2dfd3cd421e939a4aec9a58dcd upstream.
    
    Before we rework the "pmem api" to stop abusing __copy_user_nocache()
    for memcpy_to_pmem() we need to fix cases where we may strand dirty data
    in the cpu cache. The problem occurs when copy_from_iter_pmem() is used
    for arbitrary data transfers from userspace. There is no guarantee that
    these transfers, performed by dax_iomap_actor(), will have aligned
    destinations or aligned transfer lengths. Backstop the usage
    __copy_user_nocache() with explicit cache management in these unaligned
    cases.
    
    Yes, copy_from_iter_pmem() is now too big for an inline, but addressing
    that is saved for a later patch that moves the entirety of the "pmem
    api" into the pmem driver directly.
    
    Fixes: 5de490daec8b ("pmem: add copy_from_iter_pmem() and clear_pmem()")
    Cc: <x86@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5693f3fb5a662ab0ab1f8ad3a0e13c820c4c47dc
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Dec 7 01:16:27 2016 -0800

    hv: don't reset hv_context.tsc_page on crash
    
    commit 56ef6718a1d8d77745033c5291e025ce18504159 upstream.
    
    It may happen that secondary CPUs are still alive and resetting
    hv_context.tsc_page will cause a consequent crash in read_hv_clock_tsc()
    as we don't check for it being not NULL there. It is safe as we're not
    freeing this page anyways.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 03e2fb9b5ce80aa9ff6384384f5fbde156550971
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Aug 24 16:23:10 2016 -0700

    Drivers: hv: balloon: account for gaps in hot add regions
    
    commit cb7a5724c7e1bfb5766ad1c3beba14cc715991cf upstream.
    
    I'm observing the following hot add requests from the WS2012 host:
    
    hot_add_req: start_pfn = 0x108200 count = 330752
    hot_add_req: start_pfn = 0x158e00 count = 193536
    hot_add_req: start_pfn = 0x188400 count = 239616
    
    As the host doesn't specify hot add regions we're trying to create
    128Mb-aligned region covering the first request, we create the 0x108000 -
    0x160000 region and we add 0x108000 - 0x158e00 memory. The second request
    passes the pfn_covered() check, we enlarge the region to 0x108000 -
    0x190000 and add 0x158e00 - 0x188200 memory. The problem emerges with the
    third request as it starts at 0x188400 so there is a 0x200 gap which is
    not covered. As the end of our region is 0x190000 now it again passes the
    pfn_covered() check were we just adjust the covered_end_pfn and make it
    0x188400 instead of 0x188200 which means that we'll try to online
    0x188200-0x188400 pages but these pages were never assigned to us and we
    crash.
    
    We can't react to such requests by creating new hot add regions as it may
    happen that the whole suggested range falls into the previously identified
    128Mb-aligned area so we'll end up adding nothing or create intersecting
    regions and our current logic doesn't allow that. Instead, create a list of
    such 'gaps' and check for them in the page online callback.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8e7a6dbc3b71f37fc0167dde0e7676b4cdde1963
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Aug 24 16:23:09 2016 -0700

    Drivers: hv: balloon: keep track of where ha_region starts
    
    commit 7cf3b79ec85ee1a5bbaaf936bb1d050dc652983b upstream.
    
    Windows 2012 (non-R2) does not specify hot add region in hot add requests
    and the logic in hot_add_req() is trying to find a 128Mb-aligned region
    covering the request. It may also happen that host's requests are not 128Mb
    aligned and the created ha_region will start before the first specified
    PFN. We can't online these non-present pages but we don't remember the real
    start of the region.
    
    This is a regression introduced by the commit 5abbbb75d733 ("Drivers: hv:
    hv_balloon: don't lose memory when onlining order is not natural"). While
    the idea of keeping the 'moving window' was wrong (as there is no guarantee
    that hot add requests come ordered) we should still keep track of
    covered_start_pfn. This is not a revert, the logic is different.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 397488e09bf2670c841b9f9d8652ce5dd1c952f4
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Jul 6 18:24:10 2016 -0700

    Tools: hv: kvp: ensure kvp device fd is closed on exec
    
    commit 26840437cbd6d3625ea6ab34e17cd34bb810c861 upstream.
    
    KVP daemon does fork()/exec() (with popen()) so we need to close our fds
    to avoid sharing them with child processes. The immediate implication of
    not doing so I see is SELinux complaining about 'ip' trying to access
    '/dev/vmbus/hv_kvp'.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2a60bb635236ead6437b37a5b4085da1e33962b1
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Apr 3 15:12:43 2017 +0100

    kvm: arm/arm64: Fix locking for kvm_free_stage2_pgd
    
    commit 8b3405e345b5a098101b0c31b264c812bba045d9 upstream.
    
    In kvm_free_stage2_pgd() we don't hold the kvm->mmu_lock while calling
    unmap_stage2_range() on the entire memory range for the guest. This could
    cause problems with other callers (e.g, munmap on a memslot) trying to
    unmap a range. And since we have to unmap the entire Guest memory range
    holding a spinlock, make sure we yield the lock if necessary, after we
    unmap each PUD range.
    
    Fixes: commit d5d8184d35c9 ("KVM: ARM: Memory virtualization setup")
    Cc: Paolo Bonzini <pbonzin@redhat.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [ Avoid vCPU starvation and lockup detector warnings ]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e2587fba99118f2f4506b37b4766d7a4cca1465e
Author: Yazen Ghannam <yazen.ghannam@amd.com>
Date:   Thu Mar 30 13:17:14 2017 +0200

    x86/mce/AMD: Give a name to MCA bank 3 when accessed with legacy MSRs
    
    commit 29f72ce3e4d18066ec75c79c857bee0618a3504b upstream.
    
    MCA bank 3 is reserved on systems pre-Fam17h, so it didn't have a name.
    However, MCA bank 3 is defined on Fam17h systems and can be accessed
    using legacy MSRs. Without a name we get a stack trace on Fam17h systems
    when trying to register sysfs files for bank 3 on kernels that don't
    recognize Scalable MCA.
    
    Call MCA bank 3 "decode_unit" since this is what it represents on
    Fam17h. This will allow kernels without SMCA support to see this bank on
    Fam17h+ and prevent the stack trace. This will not affect older systems
    since this bank is reserved on them, i.e. it'll be ignored.
    
    Tested on AMD Fam15h and Fam17h systems.
    
      WARNING: CPU: 26 PID: 1 at lib/kobject.c:210 kobject_add_internal
      kobject: (ffff88085bb256c0): attempted to be registered with empty name!
      ...
      Call Trace:
       kobject_add_internal
       kobject_add
       kobject_create_and_add
       threshold_create_device
       threshold_init_device
    
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: http://lkml.kernel.org/r/1490102285-3659-1-git-send-email-Yazen.Ghannam@amd.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6c107bba66dcc696ec56482e7c07fccfb00d75c4
Author: Ravi Bangoria <ravi.bangoria@linux.vnet.ibm.com>
Date:   Tue Apr 11 10:38:13 2017 +0530

    powerpc/kprobe: Fix oops when kprobed on 'stdu' instruction
    
    commit 9e1ba4f27f018742a1aa95d11e35106feba08ec1 upstream.
    
    If we set a kprobe on a 'stdu' instruction on powerpc64, we see a kernel
    OOPS:
    
      Bad kernel stack pointer cd93c840 at c000000000009868
      Oops: Bad kernel stack pointer, sig: 6 [#1]
      ...
      GPR00: c000001fcd93cb30 00000000cd93c840 c0000000015c5e00 00000000cd93c840
      ...
      NIP [c000000000009868] resume_kernel+0x2c/0x58
      LR [c000000000006208] program_check_common+0x108/0x180
    
    On a 64-bit system when the user probes on a 'stdu' instruction, the kernel does
    not emulate actual store in emulate_step() because it may corrupt the exception
    frame. So the kernel does the actual store operation in exception return code
    i.e. resume_kernel().
    
    resume_kernel() loads the saved stack pointer from memory using lwz, which only
    loads the low 32-bits of the address, causing the kernel crash.
    
    Fix this by loading the 64-bit value instead.
    
    Fixes: be96f63375a1 ("powerpc: Split out instruction analysis part of emulate_step()")
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.vnet.ibm.com>
    Reviewed-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Reviewed-by: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    [mpe: Change log massage, add stable tag]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 38be91ce7ea86386242a295230a517772f359df1
Author: Sebastian Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 22 17:15:21 2017 +0100

    ubi/upd: Always flush after prepared for an update
    
    commit 9cd9a21ce070be8a918ffd3381468315a7a76ba6 upstream.
    
    In commit 6afaf8a484cb ("UBI: flush wl before clearing update marker") I
    managed to trigger and fix a similar bug. Now here is another version of
    which I assumed it wouldn't matter back then but it turns out UBI has a
    check for it and will error out like this:
    
    |ubi0 warning: validate_vid_hdr: inconsistent used_ebs
    |ubi0 error: validate_vid_hdr: inconsistent VID header at PEB 592
    
    All you need to trigger this is? "ubiupdatevol /dev/ubi0_0 file" + a
    powercut in the middle of the operation.
    ubi_start_update() sets the update-marker and puts all EBs on the erase
    list. After that userland can proceed to write new data while the old EB
    aren't erased completely. A powercut at this point is usually not that
    much of a tragedy. UBI won't give read access to the static volume
    because it has the update marker. It will most likely set the corrupted
    flag because it misses some EBs.
    So we are all good. Unless the size of the image that has been written
    differs from the old image in the magnitude of at least one EB. In that
    case UBI will find two different values for `used_ebs' and refuse to
    attach the image with the error message mentioned above.
    
    So in order not to get in the situation, the patch will ensure that we
    wait until everything is removed before it tries to write any data.
    The alternative would be to detect such a case and remove all EBs at the
    attached time after we processed the volume-table and see the
    update-marker set. The patch looks bigger and I doubt it is worth it
    since usually the write() will wait from time to time for a new EB since
    usually there not that many spare EB that can be used.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b812c69019e421cf9b4fd7e57747e73e2b83c741
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Thu Apr 20 21:32:16 2017 +0200

    mac80211: reject ToDS broadcast data frames
    
    commit 3018e947d7fd536d57e2b550c33e456d921fff8c upstream.
    
    AP/AP_VLAN modes don't accept any real 802.11 multicast data
    frames, but since they do need to accept broadcast management
    frames the same is currently permitted for data frames. This
    opens a security problem because such frames would be decrypted
    with the GTK, and could even contain unicast L3 frames.
    
    Since the spec says that ToDS frames must always have the BSSID
    as the RA (addr1), reject any other data frames.
    
    The problem was originally reported in "Predicting, Decrypting,
    and Abusing WPA2/802.11 Group Keys" at usenix
    https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/vanhoef
    and brought to my attention by Jouni.
    
    Reported-by: Jouni Malinen <j@w1.fi>
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    --

commit b74ba9dd91e53a3f182e7a2a9cefe709743c7a5f
Author: Haibo Chen <haibo.chen@nxp.com>
Date:   Wed Apr 19 10:53:51 2017 +0800

    mmc: sdhci-esdhc-imx: increase the pad I/O drive strength for DDR50 card
    
    commit 9f327845358d3dd0d8a5a7a5436b0aa5c432e757 upstream.
    
    Currently for DDR50 card, it need tuning in default. We meet tuning fail
    issue for DDR50 card and some data CRC error when DDR50 sd card works.
    
    This is because the default pad I/O drive strength can't make sure DDR50
    card work stable. So increase the pad I/O drive strength for DDR50 card,
    and use pins_100mhz.
    
    This fixes DDR50 card support for IMX since DDR50 tuning was enabled from
    commit 9faac7b95ea4 ("mmc: sdhci: enable tuning for DDR50")
    
    Tested-and-reported-by: Tim Harvey <tharvey@gateworks.com>
    Signed-off-by: Haibo Chen <haibo.chen@nxp.com>
    Acked-by: Dong Aisheng <aisheng.dong@nxp.com>
    Acked-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6986d0d29f3cda9f558461202d86464403454574
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Apr 19 19:47:04 2017 +0200

    ACPI / power: Avoid maybe-uninitialized warning
    
    commit fe8c470ab87d90e4b5115902dd94eced7e3305c3 upstream.
    
    gcc -O2 cannot always prove that the loop in acpi_power_get_inferred_state()
    is enterered at least once, so it assumes that cur_state might not get
    initialized:
    
    drivers/acpi/power.c: In function 'acpi_power_get_inferred_state':
    drivers/acpi/power.c:222:9: error: 'cur_state' may be used uninitialized in this function [-Werror=maybe-uninitialized]
    
    This sets the variable to zero at the start of the loop, to ensure that
    there is well-defined behavior even for an empty list. This gets rid of
    the warning.
    
    The warning first showed up when the -Os flag got removed in a bug fix
    patch in linux-4.11-rc5.
    
    I would suggest merging this addon patch on top of that bug fix to avoid
    introducing a new warning in the stable kernels.
    
    Fixes: 61b79e16c68d (ACPI: Fix incompatibility with mcount-based function graph tracing)
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cdede60d6a308a311b0999b826fa4cc5261632c8
Author: Thorsten Leemhuis <linux@leemhuis.info>
Date:   Tue Apr 18 11:14:28 2017 -0700

    Input: elantech - add Fujitsu Lifebook E547 to force crc_enabled
    
    commit 704de489e0e3640a2ee2d0daf173e9f7375582ba upstream.
    
    Temporary got a Lifebook E547 into my hands and noticed the touchpad
    only works after running:
    
            echo "1" > /sys/devices/platform/i8042/serio2/crc_enabled
    
    Add it to the list of machines that need this workaround.
    
    Signed-off-by: Thorsten Leemhuis <linux@leemhuis.info>
    Reviewed-by: Ulrik De Bie <ulrik.debie-os@e2big.org>
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8d5ed79fb2d766e57b5220a3e6054d99f1c985d0
Author: Jorgen Hansen <jhansen@vmware.com>
Date:   Tue Apr 5 01:59:32 2016 -0700

    VSOCK: Detach QP check should filter out non matching QPs.
    
    commit 8ab18d71de8b07d2c4d6f984b718418c09ea45c5 upstream.
    
    The check in vmci_transport_peer_detach_cb should only allow a
    detach when the qp handle of the transport matches the one in
    the detach message.
    
    Testing: Before this change, a detach from a peer on a different
    socket would cause an active stream socket to register a detach.
    
    Reviewed-by: George Zhang <georgezhang@vmware.com>
    Signed-off-by: Jorgen Hansen <jhansen@vmware.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f803416632b5c31e647cf18861e4c379173a02e2
Author: K. Y. Srinivasan <kys@microsoft.com>
Date:   Fri Jul 1 16:26:36 2016 -0700

    Drivers: hv: vmbus: Reduce the delay between retries in vmbus_post_msg()
    
    commit 8de0d7e951826d7592e0ba1da655b175c4aa0923 upstream.
    
    The current delay between retries is unnecessarily high and is negatively
    affecting the time it takes to boot the system.
    
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 567dd48c4e71a8d6d3014adb153993ef8608722c
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Jun 9 17:08:56 2016 -0700

    Drivers: hv: get rid of timeout in vmbus_open()
    
    commit 396e287fa2ff46e83ae016cdcb300c3faa3b02f6 upstream.
    
    vmbus_teardown_gpadl() can result in infinite wait when it is called on 5
    second timeout in vmbus_open(). The issue is caused by the fact that gpadl
    teardown operation won't ever succeed for an opened channel and the timeout
    isn't always enough. As a guest, we can always trust the host to respond to
    our request (and there is nothing we can do if it doesn't).
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5ab982a01201749f49b5a6a23b45b20a03490ce5
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Fri Jun 3 17:09:24 2016 -0700

    Drivers: hv: don't leak memory in vmbus_establish_gpadl()
    
    commit 7cc80c98070ccc7940fc28811c92cca0a681015d upstream.
    
    In some cases create_gpadl_header() allocates submessages but we never
    free them.
    
    [sumits] Note for stable:
    Upstream commit 4d63763296ab7865a98bc29cc7d77145815ef89f:
    (Drivers: hv: get rid of redundant messagecount in create_gpadl_header())
    changes the list usage to initialize list header in all cases; that patch
    isn't added to stable, so the current patch is modified a little bit from
    the upstream commit to check if the list is valid or not.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 702db976b8574745b20492fc2a89abfa4ec2bef1
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Sun Apr 9 22:09:38 2017 +0200

    s390/mm: fix CMMA vs KSM vs others
    
    commit a8f60d1fadf7b8b54449fcc9d6b15248917478ba upstream.
    
    On heavy paging with KSM I see guest data corruption. Turns out that
    KSM will add pages to its tree, where the mapping return true for
    pte_unused (or might become as such later).  KSM will unmap such pages
    and reinstantiate with different attributes (e.g. write protected or
    special, e.g. in replace_page or write_protect_page)). This uncovered
    a bug in our pagetable handling: We must remove the unused flag as
    soon as an entry becomes present again.
    
    Signed-of-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 859d615b5be1a6123b68f08233e548ee7f4e4298
Author: Germano Percossi <germano.percossi@citrix.com>
Date:   Fri Apr 7 12:29:37 2017 +0100

    CIFS: remove bad_network_name flag
    
    commit a0918f1ce6a43ac980b42b300ec443c154970979 upstream.
    
    STATUS_BAD_NETWORK_NAME can be received during node failover,
    causing the flag to be set and making the reconnect thread
    always unsuccessful, thereafter.
    
    Once the only place where it is set is removed, the remaining
    bits are rendered moot.
    
    Removing it does not prevent "mount" from failing when a non
    existent share is passed.
    
    What happens when the share really ceases to exist while the
    share is mounted is undefined now as much as it was before.
    
    Signed-off-by: Germano Percossi <germano.percossi@citrix.com>
    Reviewed-by: Pavel Shilovsky <pshilov@microsoft.com>
    Signed-off-by: Steve French <smfrench@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f8fe51c86583ccb38263277ee471f77053a9482a
Author: Sachin Prabhu <sprabhu@redhat.com>
Date:   Sun Apr 16 20:37:24 2017 +0100

    cifs: Do not send echoes before Negotiate is complete
    
    commit 62a6cfddcc0a5313e7da3e8311ba16226fe0ac10 upstream.
    
    commit 4fcd1813e640 ("Fix reconnect to not defer smb3 session reconnect
    long after socket reconnect") added support for Negotiate requests to
    be initiated by echo calls.
    
    To avoid delays in calling echo after a reconnect, I added the patch
    introduced by the commit b8c600120fc8 ("Call echo service immediately
    after socket reconnect").
    
    This has however caused a regression with cifs shares which do not have
    support for echo calls to trigger Negotiate requests. On connections
    which need to call Negotiation, the echo calls trigger an error which
    triggers a reconnect which in turn triggers another echo call. This
    results in a loop which is only broken when an operation is performed on
    the cifs share. For an idle share, it can DOS a server.
    
    The patch uses the smb_operation can_echo() for cifs so that it is
    called only if connection has been already been setup.
    
    kernel bz: 194531
    
    Signed-off-by: Sachin Prabhu <sprabhu@redhat.com>
    Tested-by: Jonathan Liu <net147@gmail.com>
    Acked-by: Pavel Shilovsky <pshilov@microsoft.com>
    Signed-off-by: Steve French <smfrench@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a2a67e53f92f9a3b5d94671b17b043eb56ec79ab
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 19 14:29:46 2017 -0400

    ring-buffer: Have ring_buffer_iter_empty() return true when empty
    
    commit 78f7a45dac2a2d2002f98a3a95f7979867868d73 upstream.
    
    I noticed that reading the snapshot file when it is empty no longer gives a
    status. It suppose to show the status of the snapshot buffer as well as how
    to allocate and use it. For example:
    
     ># cat snapshot
     # tracer: nop
     #
     #
     # * Snapshot is allocated *
     #
     # Snapshot commands:
     # echo 0 > snapshot : Clears and frees snapshot buffer
     # echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.
     #                      Takes a snapshot of the main buffer.
     # echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)
     #                      (Doesn't have to be '2' works with any number that
     #                       is not a '0' or '1')
    
    But instead it just showed an empty buffer:
    
     ># cat snapshot
     # tracer: nop
     #
     # entries-in-buffer/entries-written: 0/0   #P:4
     #
     #                              _-----=> irqs-off
     #                             / _----=> need-resched
     #                            | / _---=> hardirq/softirq
     #                            || / _--=> preempt-depth
     #                            ||| /     delay
     #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
     #              | |       |   ||||       |         |
    
    What happened was that it was using the ring_buffer_iter_empty() function to
    see if it was empty, and if it was, it showed the status. But that function
    was returning false when it was empty. The reason was that the iter header
    page was on the reader page, and the reader page was empty, but so was the
    buffer itself. The check only tested to see if the iter was on the commit
    page, but the commit page was no longer pointing to the reader page, but as
    all pages were empty, the buffer is also.
    
    Fixes: 651e22f2701b ("ring-buffer: Always reset iterator to reader page")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1dfb1c7bd63f7ea8975f32ddd04a9c4406f8d64d
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 19 12:07:08 2017 -0400

    tracing: Allocate the snapshot buffer before enabling probe
    
    commit df62db5be2e5f070ecd1a5ece5945b590ee112e0 upstream.
    
    Currently the snapshot trigger enables the probe and then allocates the
    snapshot. If the probe triggers before the allocation, it could cause the
    snapshot to fail and turn tracing off. It's best to allocate the snapshot
    buffer first, and then enable the trigger. If something goes wrong in the
    enabling of the trigger, the snapshot buffer is still allocated, but it can
    also be freed by the user by writting zero into the snapshot buffer file.
    
    Also add a check of the return status of alloc_snapshot().
    
    Fixes: 77fd5c15e3 ("tracing: Add snapshot trigger to function probes")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c9460fbceb2f3efa1d20050cdbffa51ec025745a
Author: Eric Biggers <ebiggers@google.com>
Date:   Tue Apr 18 15:31:09 2017 +0100

    KEYS: fix keyctl_set_reqkey_keyring() to not leak thread keyrings
    
    commit c9f838d104fed6f2f61d68164712e3204bf5271b upstream.
    
    This fixes CVE-2017-7472.
    
    Running the following program as an unprivileged user exhausts kernel
    memory by leaking thread keyrings:
    
            #include <keyutils.h>
    
            int main()
            {
                    for (;;)
                            keyctl_set_reqkey_keyring(KEY_REQKEY_DEFL_THREAD_KEYRING);
            }
    
    Fix it by only creating a new thread keyring if there wasn't one before.
    To make things more consistent, make install_thread_keyring_to_cred()
    and install_process_keyring_to_cred() both return 0 if the corresponding
    keyring is already present.
    
    Fixes: d84f4f992cbd ("CRED: Inaugurate COW credentials")
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit eb78d987757967749d0b2e82fce0314697937ee5
Author: David Howells <dhowells@redhat.com>
Date:   Tue Apr 18 15:31:08 2017 +0100

    KEYS: Change the name of the dead type to ".dead" to prevent user access
    
    commit c1644fe041ebaf6519f6809146a77c3ead9193af upstream.
    
    This fixes CVE-2017-6951.
    
    Userspace should not be able to do things with the "dead" key type as it
    doesn't have some of the helper functions set upon it that the kernel
    needs.  Attempting to use it may cause the kernel to crash.
    
    Fix this by changing the name of the type to ".dead" so that it's rejected
    up front on userspace syscalls by key_get_type_from_user().
    
    Though this doesn't seem to affect recent kernels, it does affect older
    ones, certainly those prior to:
    
            commit c06cfb08b88dfbe13be44a69ae2fdc3a7c902d81
            Author: David Howells <dhowells@redhat.com>
            Date:   Tue Sep 16 17:36:06 2014 +0100
            KEYS: Remove key_type::match in favour of overriding default by match_preparse
    
    which went in before 3.18-rc1.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b5737b92560efcb956d2def4dcd3f4b6d4118e58
Author: David Howells <dhowells@redhat.com>
Date:   Tue Apr 18 15:31:07 2017 +0100

    KEYS: Disallow keyrings beginning with '.' to be joined as session keyrings
    
    commit ee8f844e3c5a73b999edf733df1c529d6503ec2f upstream.
    
    This fixes CVE-2016-9604.
    
    Keyrings whose name begin with a '.' are special internal keyrings and so
    userspace isn't allowed to create keyrings by this name to prevent
    shadowing.  However, the patch that added the guard didn't fix
    KEYCTL_JOIN_SESSION_KEYRING.  Not only can that create dot-named keyrings,
    it can also subscribe to them as a session keyring if they grant SEARCH
    permission to the user.
    
    This, for example, allows a root process to set .builtin_trusted_keys as
    its session keyring, at which point it has full access because now the
    possessor permissions are added.  This permits root to add extra public
    keys, thereby bypassing module verification.
    
    This also affects kexec and IMA.
    
    This can be tested by (as root):
    
            keyctl session .builtin_trusted_keys
            keyctl add user a a @s
            keyctl list @s
    
    which on my test box gives me:
    
            2 keys in keyring:
            180010936: ---lswrv     0     0 asymmetric: Build time autogenerated kernel key: ae3d4a31b82daa8e1a75b49dc2bba949fd992a05
            801382539: --alswrv     0     0 user: a
    
    
    Fix this by rejecting names beginning with a '.' in the keyctl.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Mimi Zohar <zohar@linux.vnet.ibm.com>
    cc: linux-ima-devel@lists.sourceforge.net
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
