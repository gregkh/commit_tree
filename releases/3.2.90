commit 9b733a81d6cd9a4abce30e42e8b180a4dcf4d1b7
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sun Jul 2 17:12:48 2017 +0100

    Linux 3.2.90

commit 0e49b7dd019da207f0cae45962175f9996682cb7
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Sun Mar 4 19:52:03 2012 -0500

    mm: fix find_vma_prev
    
    commit 83cd904d271ba960c53f3adbb037f3486518f1e6 upstream.
    
    Commit 6bd4837de96e ("mm: simplify find_vma_prev()") broke memory
    management on PA-RISC.
    
    After application of the patch, programs that allocate big arrays on the
    stack crash with segfault, for example, this will crash if compiled
    without optimization:
    
      int main()
      {
            char array[200000];
            array[199999] = 0;
            return 0;
      }
    
    The reason is that PA-RISC has up-growing stack and the stack is usually
    the last memory area.  In the above example, a page fault happens above
    the stack.
    
    Previously, if we passed too high address to find_vma_prev, it returned
    NULL and stored the last VMA in *pprev.  After "simplify find_vma_prev"
    change, it stores NULL in *pprev.  Consequently, the stack area is not
    found and it is not expanded, as it used to be before the change.
    
    This patch restores the old behavior and makes it return the last VMA in
    *pprev if the requested address is higher than address of any other VMA.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 2e8d6ed89dc84f4f7a82c57e10125f6fc3c98d9d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jan 10 15:08:07 2012 -0800

    mm: simplify find_vma_prev()
    
    commit 6bd4837de96e7d9f9bf33e59117c24fc230862ac upstream.
    
    commit 297c5eee37 ("mm: make the vma list be doubly linked") added the
    vm_prev member to vm_area_struct.  We can simplify find_vma_prev() by
    using it.  Also, this change helps to improve page fault performance
    because it has stronger locality of reference.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 09c9faacebb3c1e279ec962cff3072995328ca29
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 15 00:12:24 2017 +0100

    rxrpc: Fix several cases where a padded len isn't checked in ticket decode
    
    commit 5f2f97656ada8d811d3c1bef503ced266fcd53a0 upstream.
    
    This fixes CVE-2017-7482.
    
    When a kerberos 5 ticket is being decoded so that it can be loaded into an
    rxrpc-type key, there are several places in which the length of a
    variable-length field is checked to make sure that it's not going to
    overrun the available data - but the data is padded to the nearest
    four-byte boundary and the code doesn't check for this extra.  This could
    lead to the size-remaining variable wrapping and the data pointer going
    over the end of the buffer.
    
    Fix this by making the various variable-length data checks use the padded
    length.
    
    Reported-by: 石磊 <shilei-c@360.cn>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Marc Dionne <marc.c.dionne@auristor.com>
    Reviewed-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    [bwh: Backported to 3.2: adjust filename, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit a7d519473a32267e52f1f92141240451e5403dd3
Author: Helge Deller <deller@gmx.de>
Date:   Mon Jun 19 17:34:05 2017 +0200

    Allow stack to grow up to address space limit
    
    commit bd726c90b6b8ce87602208701b208a208e6d5600 upstream.
    
    Fix expand_upwards() on architectures with an upward-growing stack (parisc,
    metag and partly IA-64) to allow the stack to reliably grow exactly up to
    the address space limit given by TASK_SIZE.
    
    Signed-off-by: Helge Deller <deller@gmx.de>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 640c7dfdc7c723143b1ce42f5569ec8565cbbde7
Author: Hugh Dickins <hughd@google.com>
Date:   Mon Jun 19 20:32:47 2017 +0200

    mm: larger stack guard gap, between vmas
    
    commit 1be7107fbe18eed3e319a6c3e83c78254b693acb upstream.
    
    Stack guard page is a useful feature to reduce a risk of stack smashing
    into a different mapping. We have been using a single page gap which
    is sufficient to prevent having stack adjacent to a different mapping.
    But this seems to be insufficient in the light of the stack usage in
    userspace. E.g. glibc uses as large as 64kB alloca() in many commonly
    used functions. Others use constructs liks gid_t buffer[NGROUPS_MAX]
    which is 256kB or stack strings with MAX_ARG_STRLEN.
    
    This will become especially dangerous for suid binaries and the default
    no limit for the stack size limit because those applications can be
    tricked to consume a large portion of the stack and a single glibc call
    could jump over the guard page. These attacks are not theoretical,
    unfortunatelly.
    
    Make those attacks less probable by increasing the stack guard gap
    to 1MB (on systems with 4k pages; but make it depend on the page size
    because systems with larger base pages might cap stack allocations in
    the PAGE_SIZE units) which should cover larger alloca() and VLA stack
    allocations. It is obviously not a full fix because the problem is
    somehow inherent, but it should reduce attack space a lot.
    
    One could argue that the gap size should be configurable from userspace,
    but that can be done later when somebody finds that the new 1MB is wrong
    for some special case applications.  For now, add a kernel command line
    option (stack_guard_gap) to specify the stack gap size (in page units).
    
    Implementation wise, first delete all the old code for stack guard page:
    because although we could get away with accounting one extra page in a
    stack vma, accounting a larger gap can break userspace - case in point,
    a program run with "ulimit -S -v 20000" failed when the 1MB gap was
    counted for RLIMIT_AS; similar problems could come with RLIMIT_MLOCK
    and strict non-overcommit mode.
    
    Instead of keeping gap inside the stack vma, maintain the stack guard
    gap as a gap between vmas: using vm_start_gap() in place of vm_start
    (or vm_end_gap() in place of vm_end if VM_GROWSUP) in just those few
    places which need to respect the gap - mainly arch_get_unmapped_area(),
    and and the vma tree's subtree_gap support for that.
    
    Original-patch-by: Oleg Nesterov <oleg@redhat.com>
    Original-patch-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Helge Deller <deller@gmx.de> # parisc
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    [Hugh Dickins: Backported to 3.2]
    [bwh: Fix more instances of vma->vm_start in sparc64 impl. of
     arch_get_unmapped_area_topdown() and generic impl. of
     hugetlb_get_unmapped_area()]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 641fbf5980efa6a6a2d820344eb03959e29afb6f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 27 08:36:04 2013 -0800

    mm: do not grow the stack vma just because of an overrun on preceding vma
    
    commit 09884964335e85e897876d17783c2ad33cf8a2e0 upstream.
    
    The stack vma is designed to grow automatically (marked with VM_GROWSUP
    or VM_GROWSDOWN depending on architecture) when an access is made beyond
    the existing boundary.  However, particularly if you have not limited
    your stack at all ("ulimit -s unlimited"), this can cause the stack to
    grow even if the access was really just one past *another* segment.
    
    And that's wrong, especially since we first grow the segment, but then
    immediately later enforce the stack guard page on the last page of the
    segment.  So _despite_ first growing the stack segment as a result of
    the access, the kernel will then make the access cause a SIGSEGV anyway!
    
    So do the same logic as the guard page check does, and consider an
    access to within one page of the next segment to be a bad access, rather
    than growing the stack to abut the next segment.
    
    Reported-and-tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
