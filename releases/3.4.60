commit f571d16dee723cb888ce2bce2217cfa8c2ccebfe
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Thu Aug 29 09:50:45 2013 -0700

    Linux 3.4.60

commit fc431b044608688678d871111f9c71672279390a
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Aug 16 15:42:55 2013 +0100

    x86/xen: do not identity map UNUSABLE regions in the machine E820
    
    commit 3bc38cbceb85881a8eb789ee1aa56678038b1909 upstream.
    
    If there are UNUSABLE regions in the machine memory map, dom0 will
    attempt to map them 1:1 which is not permitted by Xen and the kernel
    will crash.
    
    There isn't anything interesting in the UNUSABLE region that the dom0
    kernel needs access to so we can avoid making the 1:1 mapping and
    treat it as RAM.
    
    We only do this for dom0, as that is where tboot case shows up.
    A PV domU could have an UNUSABLE region in its pseudo-physical map
    and would need to be handled in another patch.
    
    This fixes a boot failure on hosts with tboot.
    
    tboot marks a region in the e820 map as unusable and the dom0 kernel
    would attempt to map this region and Xen does not permit unusable
    regions to be mapped by guests.
    
      (XEN)  0000000000000000 - 0000000000060000 (usable)
      (XEN)  0000000000060000 - 0000000000068000 (reserved)
      (XEN)  0000000000068000 - 000000000009e000 (usable)
      (XEN)  0000000000100000 - 0000000000800000 (usable)
      (XEN)  0000000000800000 - 0000000000972000 (unusable)
    
    tboot marked this region as unusable.
    
      (XEN)  0000000000972000 - 00000000cf200000 (usable)
      (XEN)  00000000cf200000 - 00000000cf38f000 (reserved)
      (XEN)  00000000cf38f000 - 00000000cf3ce000 (ACPI data)
      (XEN)  00000000cf3ce000 - 00000000d0000000 (reserved)
      (XEN)  00000000e0000000 - 00000000f0000000 (reserved)
      (XEN)  00000000fe000000 - 0000000100000000 (reserved)
      (XEN)  0000000100000000 - 0000000630000000 (usable)
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    [v1: Altered the patch and description with domU's with UNUSABLE regions]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3c305356ad061ee782fbcb93ed02a9bac6b7be43
Author: Martin Peschke <mpeschke@linux.vnet.ibm.com>
Date:   Thu Aug 22 17:45:37 2013 +0200

    SCSI: zfcp: fix schedule-inside-lock in scsi_device list loops
    
    commit 924dd584b198a58aa7cb3efefd8a03326550ce8f upstream.
    
    BUG: sleeping function called from invalid context at kernel/workqueue.c:2752
    in_atomic(): 1, irqs_disabled(): 1, pid: 360, name: zfcperp0.0.1700
    CPU: 1 Not tainted 3.9.3+ #69
    Process zfcperp0.0.1700 (pid: 360, task: 0000000075b7e080, ksp: 000000007476bc30)
    <snip>
    Call Trace:
    ([<00000000001165de>] show_trace+0x106/0x154)
     [<00000000001166a0>] show_stack+0x74/0xf4
     [<00000000006ff646>] dump_stack+0xc6/0xd4
     [<000000000017f3a0>] __might_sleep+0x128/0x148
     [<000000000015ece8>] flush_work+0x54/0x1f8
     [<00000000001630de>] __cancel_work_timer+0xc6/0x128
     [<00000000005067ac>] scsi_device_dev_release_usercontext+0x164/0x23c
     [<0000000000161816>] execute_in_process_context+0x96/0xa8
     [<00000000004d33d8>] device_release+0x60/0xc0
     [<000000000048af48>] kobject_release+0xa8/0x1c4
     [<00000000004f4bf2>] __scsi_iterate_devices+0xfa/0x130
     [<000003ff801b307a>] zfcp_erp_strategy+0x4da/0x1014 [zfcp]
     [<000003ff801b3caa>] zfcp_erp_thread+0xf6/0x2b0 [zfcp]
     [<000000000016b75a>] kthread+0xf2/0xfc
     [<000000000070c9de>] kernel_thread_starter+0x6/0xc
     [<000000000070c9d8>] kernel_thread_starter+0x0/0xc
    
    Apparently, the ref_count for some scsi_device drops down to zero,
    triggering device removal through execute_in_process_context(), while
    the lldd error recovery thread iterates through a scsi device list.
    Unfortunately, execute_in_process_context() decides to immediately
    execute that device removal function, instead of scheduling asynchronous
    execution, since it detects process context and thinks it is safe to do
    so. But almost all calls to shost_for_each_device() in our lldd are
    inside spin_lock_irq, even in thread context. Obviously, schedule()
    inside spin_lock_irq sections is a bad idea.
    
    Change the lldd to use the proper iterator function,
    __shost_for_each_device(), in combination with required locking.
    
    Occurences that need to be changed include all calls in zfcp_erp.c,
    since those might be executed in zfcp error recovery thread context
    with a lock held.
    
    Other occurences of shost_for_each_device() in zfcp_fsf.c do not
    need to be changed (no process context, no surrounding locking).
    
    The problem was introduced in Linux 2.6.37 by commit
    b62a8d9b45b971a67a0f8413338c230e3117dff5
    "[SCSI] zfcp: Use SCSI device data zfcp_scsi_dev instead of zfcp_unit".
    
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Peschke <mpeschke@linux.vnet.ibm.com>
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 09c756513ab486569683c6496e3285892a4e5ea0
Author: Martin Peschke <mpeschke@linux.vnet.ibm.com>
Date:   Thu Aug 22 17:45:36 2013 +0200

    SCSI: zfcp: fix lock imbalance by reworking request queue locking
    
    commit d79ff142624e1be080ad8d09101f7004d79c36e1 upstream.
    
    This patch adds wait_event_interruptible_lock_irq_timeout(), which is a
    straight-forward descendant of wait_event_interruptible_timeout() and
    wait_event_interruptible_lock_irq().
    
    The zfcp driver used to call wait_event_interruptible_timeout()
    in combination with some intricate and error-prone locking. Using
    wait_event_interruptible_lock_irq_timeout() as a replacement
    nicely cleans up that locking.
    
    This rework removes a situation that resulted in a locking imbalance
    in zfcp_qdio_sbal_get():
    
    BUG: workqueue leaked lock or atomic: events/1/0xffffff00/10
        last function: zfcp_fc_wka_port_offline+0x0/0xa0 [zfcp]
    
    It was introduced by commit c2af7545aaff3495d9bf9a7608c52f0af86fb194
    "[SCSI] zfcp: Do not wait for SBALs on stopped queue", which had a new
    code path related to ZFCP_STATUS_ADAPTER_QDIOUP that took an early exit
    without a required lock being held. The problem occured when a
    special, non-SCSI I/O request was being submitted in process context,
    when the adapter's queues had been torn down. In this case the bug
    surfaced when the Fibre Channel port connection for a well-known address
    was closed during a concurrent adapter shut-down procedure, which is a
    rare constellation.
    
    This patch also fixes these warnings from the sparse tool (make C=1):
    
    drivers/s390/scsi/zfcp_qdio.c:224:12: warning: context imbalance in
     'zfcp_qdio_sbal_check' - wrong count at exit
    drivers/s390/scsi/zfcp_qdio.c:244:5: warning: context imbalance in
     'zfcp_qdio_sbal_get' - unexpected unlock
    
    Last but not least, we get rid of that crappy lock-unlock-lock
    sequence at the beginning of the critical section.
    
    It is okay to call zfcp_erp_adapter_reopen() with req_q_lock held.
    
    Reported-by: Mikulas Patocka <mpatocka@redhat.com>
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Peschke <mpeschke@linux.vnet.ibm.com>
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 41f2be6744087bbcf8444499ac2bb36af1ae8316
Author: Terry Suereth <terry.suereth@gmail.com>
Date:   Sat Aug 17 15:53:12 2013 -0400

    libata: apply behavioral quirks to sil3826 PMP
    
    commit 8ffff94d20b7eb446e848e0046107d51b17a20a8 upstream.
    
    Fixing support for the Silicon Image 3826 port multiplier, by applying
    to it the same quirks applied to the Silicon Image 3726.  Specifically
    fixes the repeated timeout/reset process which previously afflicted
    the 3726, as described from line 290.  Slightly based on notes from:
    
    https://bugzilla.redhat.com/show_bug.cgi?id=890237
    
    Signed-off-by: Terry Suereth <terry.suereth@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 807b3dacb1ce79a79badbf919703d14eb14f96e3
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri Aug 9 12:52:31 2013 +0300

    Hostap: copying wrong data prism2_ioctl_giwaplist()
    
    commit 909bd5926d474e275599094acad986af79671ac9 upstream.
    
    We want the data stored in "addr" and "qual", but the extra ampersands
    mean we are copying stack data instead.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 03fec5cd1af33b7635db79542a85b03015356480
Author: Vyacheslav Dubeyko <slava@dubeyko.com>
Date:   Thu Aug 22 16:35:45 2013 -0700

    nilfs2: fix issue with counting number of bio requests for BIO_EOPNOTSUPP error detection
    
    commit 4bf93b50fd04118ac7f33a3c2b8a0a1f9fa80bc9 upstream.
    
    Fix the issue with improper counting number of flying bio requests for
    BIO_EOPNOTSUPP error detection case.
    
    The sb_nbio must be incremented exactly the same number of times as
    complete() function was called (or will be called) because
    nilfs_segbuf_wait() will call wail_for_completion() for the number of
    times set to sb_nbio:
    
      do {
          wait_for_completion(&segbuf->sb_bio_event);
      } while (--segbuf->sb_nbio > 0);
    
    Two functions complete() and wait_for_completion() must be called the
    same number of times for the same sb_bio_event.  Otherwise,
    wait_for_completion() will hang or leak.
    
    Signed-off-by: Vyacheslav Dubeyko <slava@dubeyko.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Tested-by: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6ed43927ea4cf330d5f5b0aea6b8599e354aba3e
Author: Vyacheslav Dubeyko <slava@dubeyko.com>
Date:   Thu Aug 22 16:35:44 2013 -0700

    nilfs2: remove double bio_put() in nilfs_end_bio_write() for BIO_EOPNOTSUPP error
    
    commit 2df37a19c686c2d7c4e9b4ce1505b5141e3e5552 upstream.
    
    Remove double call of bio_put() in nilfs_end_bio_write() for the case of
    BIO_EOPNOTSUPP error detection.  The issue was found by Dan Carpenter
    and he suggests first version of the fix too.
    
    Signed-off-by: Vyacheslav Dubeyko <slava@dubeyko.com>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Tested-by: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 00d0f98e9424e5df7bb06955a740b4df92c5eb3a
Author: Wladislav Wiebe <wladislav.kw@gmail.com>
Date:   Mon Aug 12 13:06:53 2013 +0200

    of: fdt: fix memory initialization for expanded DT
    
    commit 9e40127526e857fa3f29d51e83277204fbdfc6ba upstream.
    
    Already existing property flags are filled wrong for properties created from
    initial FDT. This could cause problems if this DYNAMIC device-tree functions
    are used later, i.e. properties are attached/detached/replaced. Simply dumping
    flags from the running system show, that some initial static (not allocated via
    kzmalloc()) nodes are marked as dynamic.
    
    I putted some debug extensions to property_proc_show(..) :
    ..
    +       if (OF_IS_DYNAMIC(pp))
    +               pr_err("DEBUG: xxx : OF_IS_DYNAMIC\n");
    +       if (OF_IS_DETACHED(pp))
    +               pr_err("DEBUG: xxx : OF_IS_DETACHED\n");
    
    when you operate on the nodes (e.g.: ~$ cat /proc/device-tree/*some_node*) you
    will see that those flags are filled wrong, basically in most cases it will dump
    a DYNAMIC or DETACHED status, which is in not true.
    (BTW. this OF_IS_DETACHED is a own define for debug purposes which which just
    make a test_bit(OF_DETACHED, &x->_flags)
    
    If nodes are dynamic kernel is allowed to kfree() them. But it will crash
    attempting to do so on the nodes from FDT -- they are not allocated via
    kzmalloc().
    
    Signed-off-by: Wladislav Wiebe <wladislav.kw@gmail.com>
    Acked-by: Alexander Sverdlin <alexander.sverdlin@nsn.com>
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 05dd70866930b851b02bd28dc635739387b2b8f3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Aug 6 19:01:14 2013 +0100

    drm/i915: Invalidate TLBs for the rings after a reset
    
    commit 884020bf3d2a3787a1cc6df902e98e0eec60330b upstream.
    
    After any "soft gfx reset" we must manually invalidate the TLBs
    associated with each ring. Empirically, it seems that a
    suspend/resume or D3-D0 cycle count as a "soft reset". The symptom is
    that the hardware would fail to note the new address for its status
    page, and so it would continue to write the shadow registers and
    breadcrumbs into the old physical address (now used by something
    completely different, scary). Whereas the driver would read the new
    status page and never see any progress, it would appear that the GPU
    hung immediately upon resume.
    
    Based on a patch by naresh kumar kachhi <naresh.kumar.kacchi@intel.com>
    
    Reported-by: Thiago Macieira <thiago@kde.org>
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=64725
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Tested-by: Thiago Macieira <thiago@kde.org>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a34794460aa1fb5096708d079a29606123a2ec9c
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Aug 15 13:21:06 2013 +0100

    xen/events: initialize local per-cpu mask for all possible events
    
    commit 84ca7a8e45dafb49cd5ca90a343ba033e2885c17 upstream.
    
    The sizeof() argument in init_evtchn_cpu_bindings() is incorrect
    resulting in only the first 64 (or 32 in 32-bit guests) ports having
    their bindings being initialized to VCPU 0.
    
    In most cases this does not cause a problem as request_irq() will set
    the irq affinity which will set the correct local per-cpu mask.
    However, if the request_irq() is called on a VCPU other than 0, there
    is a window between the unmasking of the event and the affinity being
    set were an event may be lost because it is not locally unmasked on
    any VCPU. If request_irq() is called on VCPU 0 then local irqs are
    disabled during the window and the race does not occur.
    
    Fix this by initializing all NR_EVENT_CHANNEL bits in the local
    per-cpu masks.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 523578f790ece2db130b23e8d2ef37e48f698317
Author: Jussi Kivilinna <jussi.kivilinna@iki.fi>
Date:   Tue Aug 6 14:28:42 2013 +0300

    zd1201: do not use stack as URB transfer_buffer
    
    commit 1206ff4ff9d2ef7468a355328bc58ac6ebf5be44 upstream.
    
    Patch fixes zd1201 not to use stack as URB transfer_buffer. URB buffers need
    to be DMA-able, which stack is not.
    
    Patch is only compile tested.
    
    Signed-off-by: Jussi Kivilinna <jussi.kivilinna@iki.fi>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 55e3e1f419f0c387a2b971cc181b8dea1b099d1d
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 18 10:35:02 2012 -0800

    workqueue: consider work function when searching for busy work items
    
    commit a2c1c57be8d9fd5b716113c8991d3d702eeacf77 upstream.
    
    To avoid executing the same work item concurrenlty, workqueue hashes
    currently busy workers according to their current work items and looks
    up the the table when it wants to execute a new work item.  If there
    already is a worker which is executing the new work item, the new item
    is queued to the found worker so that it gets executed only after the
    current execution finishes.
    
    Unfortunately, a work item may be freed while being executed and thus
    recycled for different purposes.  If it gets recycled for a different
    work item and queued while the previous execution is still in
    progress, workqueue may make the new work item wait for the old one
    although the two aren't really related in any way.
    
    In extreme cases, this false dependency may lead to deadlock although
    it's extremely unlikely given that there aren't too many self-freeing
    work item users and they usually don't wait for other work items.
    
    To alleviate the problem, record the current work function in each
    busy worker and match it together with the work item address in
    find_worker_executing_work().  While this isn't complete, it ensures
    that unrelated work items don't interact with each other and in the
    very unlikely case where a twisted wq user triggers it, it's always
    onto itself making the culprit easy to spot.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Andrey Isakov <andy51@gmx.ru>
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=51701
    [lizf: Backported to 3.4:
     - Adjust context
     - Incorporate earlier logging cleanup in process_one_work() from
       044c782ce3a9 ('workqueue: fix checkpatch issues')]
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 31eafff4382b0f20edce1afea8e2d288c6c7187c
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Sep 18 10:40:00 2012 -0700

    workqueue: fix possible stall on try_to_grab_pending() of a delayed work item
    
    commit 3aa62497594430ea522050b75c033f71f2c60ee6 upstream.
    
    Currently, when try_to_grab_pending() grabs a delayed work item, it
    leaves its linked work items alone on the delayed_works.  The linked
    work items are always NO_COLOR and will cause future
    cwq_activate_first_delayed() increase cwq->nr_active incorrectly, and
    may cause the whole cwq to stall.  For example,
    
    state: cwq->max_active = 1, cwq->nr_active = 1
           one work in cwq->pool, many in cwq->delayed_works.
    
    step1: try_to_grab_pending() removes a work item from delayed_works
           but leaves its NO_COLOR linked work items on it.
    
    step2: Later on, cwq_activate_first_delayed() activates the linked
           work item increasing ->nr_active.
    
    step3: cwq->nr_active = 1, but all activated work items of the cwq are
           NO_COLOR.  When they finish, cwq->nr_active will not be
           decreased due to NO_COLOR, and no further work items will be
           activated from cwq->delayed_works. the cwq stalls.
    
    Fix it by ensuring the target work item is activated before stealing
    PENDING in try_to_grab_pending().  This ensures that all the linked
    work items are activated without incorrectly bumping cwq->nr_active.
    
    tj: Updated comment and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    [lizf: backported to 3.4: adjust context]
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
