commit fcfbdfe9626edd5bf00c732e093eed249ecdbfa1
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sun Jul 11 12:49:32 2021 +0200

    Linux 4.19.197
    
    Link: https://lore.kernel.org/r/20210709131644.969303901@linuxfoundation.org
    Tested-by: Jon Hunter <jonathanh@nvidia.com>
    Tested-by: Shuah Khan <skhan@linuxfoundation.org>
    Tested-by: Sudip Mukherjee <sudip.mukherjee@codethink.co.uk>
    Tested-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Pavel Machek (CIP) <pavel@denx.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 330584716d4b5b9653608edc0adf7251bcd7e506
Author: Tony Lindgren <tony@atomide.com>
Date:   Fri Jul 9 10:37:45 2021 +0300

    clocksource/drivers/timer-ti-dm: Handle dra7 timer wrap errata i940
    
    commit 25de4ce5ed02994aea8bc111d133308f6fd62566 upstream.
    
    There is a timer wrap issue on dra7 for the ARM architected timer.
    In a typical clock configuration the timer fails to wrap after 388 days.
    
    To work around the issue, we need to use timer-ti-dm percpu timers instead.
    
    Let's configure dmtimer3 and 4 as percpu timers by default, and warn about
    the issue if the dtb is not configured properly.
    
    For more information, please see the errata for "AM572x Sitara Processors
    Silicon Revisions 1.1, 2.0":
    
    https://www.ti.com/lit/er/sprz429m/sprz429m.pdf
    
    The concept is based on earlier reference patches done by Tero Kristo and
    Keerthy.
    
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Keerthy <j-keerthy@ti.com>
    Cc: Tero Kristo <kristo@kernel.org>
    [tony@atomide.com: backported to 4.19.y]
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 123235223169d1a73e55fcd5d857be676207021f
Author: Tony Lindgren <tony@atomide.com>
Date:   Fri Jul 9 10:37:44 2021 +0300

    clocksource/drivers/timer-ti-dm: Prepare to handle dra7 timer wrap issue
    
    commit 3efe7a878a11c13b5297057bfc1e5639ce1241ce upstream.
    
    There is a timer wrap issue on dra7 for the ARM architected timer.
    In a typical clock configuration the timer fails to wrap after 388 days.
    
    To work around the issue, we need to use timer-ti-dm timers instead.
    
    Let's prepare for adding support for percpu timers by adding a common
    dmtimer_clkevt_init_common() and call it from __omap_sync32k_timer_init().
    This patch makes no intentional functional changes.
    
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Keerthy <j-keerthy@ti.com>
    Cc: Tero Kristo <kristo@kernel.org>
    [tony@atomide.com: backported to 4.19.y]
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 78130e2e4e405d51f74cfebec47dd43ec6164277
Author: Tony Lindgren <tony@atomide.com>
Date:   Fri Jul 9 10:37:43 2021 +0300

    clocksource/drivers/timer-ti-dm: Add clockevent and clocksource support
    
    commit 52762fbd1c4778ac9b173624ca0faacd22ef4724 upstream.
    
    We can move the TI dmtimer clockevent and clocksource to live under
    drivers/clocksource if we rely only on the clock framework, and handle
    the module configuration directly in the clocksource driver based on the
    device tree data.
    
    This removes the early dependency with system timers to the interconnect
    related code, and we can probe pretty much everything else later on at
    the module_init level.
    
    Let's first add a new driver for timer-ti-dm-systimer based on existing
    arch/arm/mach-omap2/timer.c. Then let's start moving SoCs to probe with
    device tree data while still keeping the old timer.c. And eventually we
    can just drop the old timer.c.
    
    Let's take the opportunity to switch to use readl/writel as pointed out
    by Daniel Lezcano <daniel.lezcano@linaro.org>. This allows further
    clean-up of the timer-ti-dm code the a lot of the shared helpers can
    just become static to the non-syster related code.
    
    Note the boards can optionally configure different timer source clocks
    if needed with assigned-clocks and assigned-clock-parents.
    
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Keerthy <j-keerthy@ti.com>
    Cc: Tero Kristo <kristo@kernel.org>
    [tony@atomide.com: backported to 4.19.y]
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c74081df081c306d9d256d9026181b67f595fdf6
Author: afzal mohammed <afzal.mohd.ma@gmail.com>
Date:   Fri Jul 9 10:37:42 2021 +0300

    ARM: OMAP: replace setup_irq() by request_irq()
    
    commit b75ca5217743e4d7076cf65e044e88389e44318d upstream.
    
    request_irq() is preferred over setup_irq(). Invocations of setup_irq()
    occur after memory allocators are ready.
    
    Per tglx[1], setup_irq() existed in olden days when allocators were not
    ready by the time early interrupts were initialized.
    
    Hence replace setup_irq() by request_irq().
    
    [1] https://lkml.kernel.org/r/alpine.DEB.2.20.1710191609480.1971@nanos
    
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Keerthy <j-keerthy@ti.com>
    Cc: Tero Kristo <kristo@kernel.org>
    Signed-off-by: afzal mohammed <afzal.mohd.ma@gmail.com>
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4686e3e4aa78e39721f49d6a2719c61b444c7525
Author: Alper Gun <alpergun@google.com>
Date:   Thu Jun 10 17:46:04 2021 +0000

    KVM: SVM: Call SEV Guest Decommission if ASID binding fails
    
    commit 934002cd660b035b926438244b4294e647507e13 upstream.
    
    Send SEV_CMD_DECOMMISSION command to PSP firmware if ASID binding
    fails. If a failure happens after  a successful LAUNCH_START command,
    a decommission command should be executed. Otherwise, guest context
    will be unfreed inside the AMD SP. After the firmware will not have
    memory to allocate more SEV guest context, LAUNCH_START command will
    begin to fail with SEV_RET_RESOURCE_LIMIT error.
    
    The existing code calls decommission inside sev_unbind_asid, but it is
    not called if a failure happens before guest activation succeeds. If
    sev_bind_asid fails, decommission is never called. PSP firmware has a
    limit for the number of guests. If sev_asid_binding fails many times,
    PSP firmware will not have resources to create another guest context.
    
    Cc: stable@vger.kernel.org
    Fixes: 59414c989220 ("KVM: SVM: Add support for KVM_SEV_LAUNCH_START command")
    Reported-by: Peter Gonda <pgonda@google.com>
    Signed-off-by: Alper Gun <alpergun@google.com>
    Reviewed-by: Marc Orr <marcorr@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Message-Id: <20210610174604.2554090-1-alpergun@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cda326e5033fc1e7912ca31887b05fa7cd8060f3
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Jun 23 15:09:13 2021 +0200

    xen/events: reset active flag for lateeoi events later
    
    commit 3de218ff39b9e3f0d453fe3154f12a174de44b25 upstream.
    
    In order to avoid a race condition for user events when changing
    cpu affinity reset the active flag only when EOI-ing the event.
    
    This is working fine as all user events are lateeoi events. Note that
    lateeoi_ack_mask_dynirq() is not modified as there is no explicit call
    to xen_irq_lateeoi() expected later.
    
    Cc: stable@vger.kernel.org
    Reported-by: Julien Grall <julien@xen.org>
    Fixes: b6622798bc50b62 ("xen/events: avoid handling the same event on two cpus at the same time")
    Tested-by: Julien Grall <julien@xen.org>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrvsky@oracle.com>
    Link: https://lore.kernel.org/r/20210623130913.9405-1-jgross@suse.com
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6e2a98bc902ce347747fabb133485d7ce2bdd7e4
Author: Petr Mladek <pmladek@suse.com>
Date:   Thu Jun 24 18:39:48 2021 -0700

    kthread: prevent deadlock when kthread_mod_delayed_work() races with kthread_cancel_delayed_work_sync()
    
    commit 5fa54346caf67b4b1b10b1f390316ae466da4d53 upstream.
    
    The system might hang with the following backtrace:
    
            schedule+0x80/0x100
            schedule_timeout+0x48/0x138
            wait_for_common+0xa4/0x134
            wait_for_completion+0x1c/0x2c
            kthread_flush_work+0x114/0x1cc
            kthread_cancel_work_sync.llvm.16514401384283632983+0xe8/0x144
            kthread_cancel_delayed_work_sync+0x18/0x2c
            xxxx_pm_notify+0xb0/0xd8
            blocking_notifier_call_chain_robust+0x80/0x194
            pm_notifier_call_chain_robust+0x28/0x4c
            suspend_prepare+0x40/0x260
            enter_state+0x80/0x3f4
            pm_suspend+0x60/0xdc
            state_store+0x108/0x144
            kobj_attr_store+0x38/0x88
            sysfs_kf_write+0x64/0xc0
            kernfs_fop_write_iter+0x108/0x1d0
            vfs_write+0x2f4/0x368
            ksys_write+0x7c/0xec
    
    It is caused by the following race between kthread_mod_delayed_work()
    and kthread_cancel_delayed_work_sync():
    
    CPU0                            CPU1
    
    Context: Thread A               Context: Thread B
    
    kthread_mod_delayed_work()
      spin_lock()
      __kthread_cancel_work()
         spin_unlock()
         del_timer_sync()
                                    kthread_cancel_delayed_work_sync()
                                      spin_lock()
                                      __kthread_cancel_work()
                                        spin_unlock()
                                        del_timer_sync()
                                        spin_lock()
    
                                      work->canceling++
                                      spin_unlock
         spin_lock()
       queue_delayed_work()
         // dwork is put into the worker->delayed_work_list
    
       spin_unlock()
    
                                      kthread_flush_work()
         // flush_work is put at the tail of the dwork
    
                                        wait_for_completion()
    
    Context: IRQ
    
      kthread_delayed_work_timer_fn()
        spin_lock()
        list_del_init(&work->node);
        spin_unlock()
    
    BANG: flush_work is not longer linked and will never get proceed.
    
    The problem is that kthread_mod_delayed_work() checks work->canceling
    flag before canceling the timer.
    
    A simple solution is to (re)check work->canceling after
    __kthread_cancel_work().  But then it is not clear what should be
    returned when __kthread_cancel_work() removed the work from the queue
    (list) and it can't queue it again with the new @delay.
    
    The return value might be used for reference counting.  The caller has
    to know whether a new work has been queued or an existing one was
    replaced.
    
    The proper solution is that kthread_mod_delayed_work() will remove the
    work from the queue (list) _only_ when work->canceling is not set.  The
    flag must be checked after the timer is stopped and the remaining
    operations can be done under worker->lock.
    
    Note that kthread_mod_delayed_work() could remove the timer and then
    bail out.  It is fine.  The other canceling caller needs to cancel the
    timer as well.  The important thing is that the queue (list)
    manipulation is done atomically under worker->lock.
    
    Link: https://lkml.kernel.org/r/20210610133051.15337-3-pmladek@suse.com
    Fixes: 9a6b06c8d9a220860468a ("kthread: allow to modify delayed kthread work")
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Reported-by: Martin Liu <liumartin@google.com>
    Cc: <jenhaochen@google.com>
    Cc: Minchan Kim <minchan@google.com>
    Cc: Nathan Chancellor <nathan@kernel.org>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 13bcf5aeb33caa283c6b03e14b7a254e1223c0d7
Author: Petr Mladek <pmladek@suse.com>
Date:   Thu Jun 24 18:39:45 2021 -0700

    kthread_worker: split code for canceling the delayed work timer
    
    commit 34b3d5344719d14fd2185b2d9459b3abcb8cf9d8 upstream.
    
    Patch series "kthread_worker: Fix race between kthread_mod_delayed_work()
    and kthread_cancel_delayed_work_sync()".
    
    This patchset fixes the race between kthread_mod_delayed_work() and
    kthread_cancel_delayed_work_sync() including proper return value
    handling.
    
    This patch (of 2):
    
    Simple code refactoring as a preparation step for fixing a race between
    kthread_mod_delayed_work() and kthread_cancel_delayed_work_sync().
    
    It does not modify the existing behavior.
    
    Link: https://lkml.kernel.org/r/20210610133051.15337-2-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Cc: <jenhaochen@google.com>
    Cc: Martin Liu <liumartin@google.com>
    Cc: Minchan Kim <minchan@google.com>
    Cc: Nathan Chancellor <nathan@kernel.org>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4ca30ef6257a729fdb9d42a80d72984dd332bfd6
Author: Anson Huang <Anson.Huang@nxp.com>
Date:   Mon Dec 30 09:41:07 2019 +0800

    ARM: dts: imx6qdl-sabresd: Remove incorrect power supply assignment
    
    commit 4521de30fbb3f5be0db58de93582ebce72c9d44f upstream.
    
    The vdd3p0 LDO's input should be from external USB VBUS directly, NOT
    PMIC's power supply, the vdd3p0 LDO's target output voltage can be
    controlled by SW, and it requires input voltage to be high enough, with
    incorrect power supply assigned, if the power supply's voltage is lower
    than the LDO target output voltage, it will return fail and skip the LDO
    voltage adjustment, so remove the power supply assignment for vdd3p0 to
    avoid such scenario.
    
    Fixes: 93385546ba36 ("ARM: dts: imx6qdl-sabresd: Assign corresponding power supply for LDOs")
    Signed-off-by: Anson Huang <Anson.Huang@nxp.com>
    Signed-off-by: Shawn Guo <shawnguo@kernel.org>
    Signed-off-by: Nobuhiro Iwamatsu (CIP) <nobuhiro1.iwamatsu@toshiba.co.jp>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cadf5bbcefbd9717e51c61d6128b520583ffdf4f
Author: David Rientjes <rientjes@google.com>
Date:   Tue Aug 25 12:56:28 2020 -0700

    KVM: SVM: Periodically schedule when unregistering regions on destroy
    
    commit 7be74942f184fdfba34ddd19a0d995deb34d4a03 upstream.
    
    There may be many encrypted regions that need to be unregistered when a
    SEV VM is destroyed.  This can lead to soft lockups.  For example, on a
    host running 4.15:
    
    watchdog: BUG: soft lockup - CPU#206 stuck for 11s! [t_virtual_machi:194348]
    CPU: 206 PID: 194348 Comm: t_virtual_machi
    RIP: 0010:free_unref_page_list+0x105/0x170
    ...
    Call Trace:
     [<0>] release_pages+0x159/0x3d0
     [<0>] sev_unpin_memory+0x2c/0x50 [kvm_amd]
     [<0>] __unregister_enc_region_locked+0x2f/0x70 [kvm_amd]
     [<0>] svm_vm_destroy+0xa9/0x200 [kvm_amd]
     [<0>] kvm_arch_destroy_vm+0x47/0x200
     [<0>] kvm_put_kvm+0x1a8/0x2f0
     [<0>] kvm_vm_release+0x25/0x30
     [<0>] do_exit+0x335/0xc10
     [<0>] do_group_exit+0x3f/0xa0
     [<0>] get_signal+0x1bc/0x670
     [<0>] do_signal+0x31/0x130
    
    Although the CLFLUSH is no longer issued on every encrypted region to be
    unregistered, there are no other changes that can prevent soft lockups for
    very large SEV VMs in the latest kernel.
    
    Periodically schedule if necessary.  This still holds kvm->lock across the
    resched, but since this only happens when the VM is destroyed this is
    assumed to be acceptable.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Message-Id: <alpine.DEB.2.23.453.2008251255240.2987727@chino.kir.corp.google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [iwamatsu: adjust filename.]
    Reference: CVE-2020-36311
    Signed-off-by: Nobuhiro Iwamatsu (CIP) <nobuhiro1.iwamatsu@toshiba.co.jp>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6a6e04ce3bafb24a346a64e9766ec37814036245
Author: Tahsin Erdogan <trdgn@amazon.com>
Date:   Sat Jul 3 16:05:55 2021 -0700

    ext4: eliminate bogus error in ext4_data_block_valid_rcu()
    
    Mainline commit ce9f24cccdc0 ("ext4: check journal inode extents more carefully")
    enabled validity checks for journal inode's data blocks. This change got
    ported to stable branches, but the backport for 4.19 has a bug where it will
    flag an error even when system block entry's inode number matches journal
    inode.
    
    The way error is reported is also problematic because it updates the superblock
    without following journaling rules. This may result in superblock checksum
    errors if the superblock is in the process of being committed but has a
    previously calculated checksum that doesn't include the bogus error update.
    
    This patch eliminates the bogus error by trying to match how other backports
    were implemented, which is to flag an error only when inode numbers mismatch.
    
    Fixes: commit a75a5d163857 ("ext4: check journal inode extents more carefully")
    Signed-off-by: Tahsin Erdogan <trdgn@amazon.com>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6dcca74b358a849c55ab2db41ae8f97e08ffb8e0
Author: Christian König <christian.koenig@amd.com>
Date:   Fri Jun 11 14:34:50 2021 +0200

    drm/nouveau: fix dma_address check for CPU/GPU sync
    
    [ Upstream commit d330099115597bbc238d6758a4930e72b49ea9ba ]
    
    AGP for example doesn't have a dma_address array.
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20210614110517.1624-1-christian.koenig@amd.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b52a404434a93d83b243187c6cf135dd6ba529c8
Author: ManYi Li <limanyi@uniontech.com>
Date:   Fri Jun 11 17:44:02 2021 +0800

    scsi: sr: Return appropriate error code when disk is ejected
    
    [ Upstream commit 7dd753ca59d6c8cc09aa1ed24f7657524803c7f3 ]
    
    Handle a reported media event code of 3. This indicates that the media has
    been removed from the drive and user intervention is required to proceed.
    Return DISK_EVENT_EJECT_REQUEST in that case.
    
    Link: https://lore.kernel.org/r/20210611094402.23884-1-limanyi@uniontech.com
    Signed-off-by: ManYi Li <limanyi@uniontech.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2445837e9cd084ba849a7c1c70086a6cdc608f48
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:52 2021 -0700

    mm, futex: fix shared futex pgoff on shmem huge page
    
    [ Upstream commit fe19bd3dae3d15d2fbfdb3de8839a6ea0fe94264 ]
    
    If more than one futex is placed on a shmem huge page, it can happen
    that waking the second wakes the first instead, and leaves the second
    waiting: the key's shared.pgoff is wrong.
    
    When 3.11 commit 13d60f4b6ab5 ("futex: Take hugepages into account when
    generating futex_key"), the only shared huge pages came from hugetlbfs,
    and the code added to deal with its exceptional page->index was put into
    hugetlb source.  Then that was missed when 4.8 added shmem huge pages.
    
    page_to_pgoff() is what others use for this nowadays: except that, as
    currently written, it gives the right answer on hugetlbfs head, but
    nonsense on hugetlbfs tails.  Fix that by calling hugetlbfs-specific
    hugetlb_basepage_index() on PageHuge tails as well as on head.
    
    Yes, it's unconventional to declare hugetlb_basepage_index() there in
    pagemap.h, rather than in hugetlb.h; but I do not expect anything but
    page_to_pgoff() ever to need it.
    
    [akpm@linux-foundation.org: give hugetlb_basepage_index() prototype the correct scope]
    
    Link: https://lkml.kernel.org/r/b17d946b-d09-326e-b42a-52884c36df32@google.com
    Fixes: 800d8c63b2e9 ("shmem: add huge pages support")
    Reported-by: Neel Natu <neelnatu@google.com>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Zhang Yi <wetpzy@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Darren Hart <dvhart@infradead.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: leave redundant #include <linux/hugetlb.h>
    in kernel/futex.c, to avoid conflict over the header files included.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit e943b4373cf706ee8ee433988bc0c4d6e3ea5907
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:30 2021 -0700

    mm/thp: another PVMW_SYNC fix in page_vma_mapped_walk()
    
    [ Upstream commit a7a69d8ba88d8dcee7ef00e91d413a4bd003a814 ]
    
    Aha! Shouldn't that quick scan over pte_none()s make sure that it holds
    ptlock in the PVMW_SYNC case? That too might have been responsible for
    BUGs or WARNs in split_huge_page_to_list() or its unmap_page(), though
    I've never seen any.
    
    Link: https://lkml.kernel.org/r/1bdf384c-8137-a149-2a1e-475a4791c3c@google.com
    Link: https://lore.kernel.org/linux-mm/20210412180659.B9E3.409509F4@e16-tech.com/
    Fixes: ace71a19cec5 ("mm: introduce page_vma_mapped_walk()")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 69784c9d5cb080a57062d944acb34c334758ff0a
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:26 2021 -0700

    mm/thp: fix page_vma_mapped_walk() if THP mapped by ptes
    
    [ Upstream commit a9a7504d9beaf395481faa91e70e2fd08f7a3dde ]
    
    Running certain tests with a DEBUG_VM kernel would crash within hours,
    on the total_mapcount BUG() in split_huge_page_to_list(), while trying
    to free up some memory by punching a hole in a shmem huge page: split's
    try_to_unmap() was unable to find all the mappings of the page (which,
    on a !DEBUG_VM kernel, would then keep the huge page pinned in memory).
    
    Crash dumps showed two tail pages of a shmem huge page remained mapped
    by pte: ptes in a non-huge-aligned vma of a gVisor process, at the end
    of a long unmapped range; and no page table had yet been allocated for
    the head of the huge page to be mapped into.
    
    Although designed to handle these odd misaligned huge-page-mapped-by-pte
    cases, page_vma_mapped_walk() falls short by returning false prematurely
    when !pmd_present or !pud_present or !p4d_present or !pgd_present: there
    are cases when a huge page may span the boundary, with ptes present in
    the next.
    
    Restructure page_vma_mapped_walk() as a loop to continue in these cases,
    while keeping its layout much as before.  Add a step_forward() helper to
    advance pvmw->address across those boundaries: originally I tried to use
    mm's standard p?d_addr_end() macros, but hit the same crash 512 times
    less often: because of the way redundant levels are folded together, but
    folded differently in different configurations, it was just too
    difficult to use them correctly; and step_forward() is simpler anyway.
    
    Link: https://lkml.kernel.org/r/fedb8632-1798-de42-f39e-873551d5bc81@google.com
    Fixes: ace71a19cec5 ("mm: introduce page_vma_mapped_walk()")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b114408b1aa2b0957877632113009bdf5efc5da2
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:23 2021 -0700

    mm: page_vma_mapped_walk(): get vma_address_end() earlier
    
    [ Upstream commit a765c417d876cc635f628365ec9aa6f09470069a ]
    
    page_vma_mapped_walk() cleanup: get THP's vma_address_end() at the
    start, rather than later at next_pte.
    
    It's a little unnecessary overhead on the first call, but makes for a
    simpler loop in the following commit.
    
    Link: https://lkml.kernel.org/r/4542b34d-862f-7cb4-bb22-e0df6ce830a2@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7d82908ba4fb6ae809db8a5d63f85f1acc6d2946
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:20 2021 -0700

    mm: page_vma_mapped_walk(): use goto instead of while (1)
    
    [ Upstream commit 474466301dfd8b39a10c01db740645f3f7ae9a28 ]
    
    page_vma_mapped_walk() cleanup: add a label this_pte, matching next_pte,
    and use "goto this_pte", in place of the "while (1)" loop at the end.
    
    Link: https://lkml.kernel.org/r/a52b234a-851-3616-2525-f42736e8934@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ac0324b14dae4dd664fd94d9ada161089f4d2b8b
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:17 2021 -0700

    mm: page_vma_mapped_walk(): add a level of indentation
    
    [ Upstream commit b3807a91aca7d21c05d5790612e49969117a72b9 ]
    
    page_vma_mapped_walk() cleanup: add a level of indentation to much of
    the body, making no functional change in this commit, but reducing the
    later diff when this is all converted to a loop.
    
    [hughd@google.com: : page_vma_mapped_walk(): add a level of indentation fix]
      Link: https://lkml.kernel.org/r/7f817555-3ce1-c785-e438-87d8efdcaf26@google.com
    
    Link: https://lkml.kernel.org/r/efde211-f3e2-fe54-977-ef481419e7f3@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 97a79b7896d6b8c009561f695649b31c1b35437c
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:14 2021 -0700

    mm: page_vma_mapped_walk(): crossing page table boundary
    
    [ Upstream commit 448282487483d6fa5b2eeeafaa0acc681e544a9c ]
    
    page_vma_mapped_walk() cleanup: adjust the test for crossing page table
    boundary - I believe pvmw->address is always page-aligned, but nothing
    else here assumed that; and remember to reset pvmw->pte to NULL after
    unmapping the page table, though I never saw any bug from that.
    
    Link: https://lkml.kernel.org/r/799b3f9c-2a9e-dfef-5d89-26e9f76fd97@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d4b99cf445d24089255a49ff927ed8884ca2f7f5
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:10 2021 -0700

    mm: page_vma_mapped_walk(): prettify PVMW_MIGRATION block
    
    [ Upstream commit e2e1d4076c77b3671cf8ce702535ae7dee3acf89 ]
    
    page_vma_mapped_walk() cleanup: rearrange the !pmd_present() block to
    follow the same "return not_found, return not_found, return true"
    pattern as the block above it (note: returning not_found there is never
    premature, since existence or prior existence of huge pmd guarantees
    good alignment).
    
    Link: https://lkml.kernel.org/r/378c8650-1488-2edf-9647-32a53cf2e21@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9fbb45c5d59d8e4c81c90cc418bed167503db227
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:07 2021 -0700

    mm: page_vma_mapped_walk(): use pmde for *pvmw->pmd
    
    [ Upstream commit 3306d3119ceacc43ea8b141a73e21fea68eec30c ]
    
    page_vma_mapped_walk() cleanup: re-evaluate pmde after taking lock, then
    use it in subsequent tests, instead of repeatedly dereferencing pointer.
    
    Link: https://lkml.kernel.org/r/53fbc9d-891e-46b2-cb4b-468c3b19238e@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a027b699161023c017b816b61be9990e3f9286f4
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:04 2021 -0700

    mm: page_vma_mapped_walk(): settle PageHuge on entry
    
    [ Upstream commit 6d0fd5987657cb0c9756ce684e3a74c0f6351728 ]
    
    page_vma_mapped_walk() cleanup: get the hugetlbfs PageHuge case out of
    the way at the start, so no need to worry about it later.
    
    Link: https://lkml.kernel.org/r/e31a483c-6d73-a6bb-26c5-43c3b880a2@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit be9ab2d00d071f930cc0389a8ce1424fd84d4c6b
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jun 24 18:39:01 2021 -0700

    mm: page_vma_mapped_walk(): use page for pvmw->page
    
    [ Upstream commit f003c03bd29e6f46fef1b9a8e8d636ac732286d5 ]
    
    Patch series "mm: page_vma_mapped_walk() cleanup and THP fixes".
    
    I've marked all of these for stable: many are merely cleanups, but I
    think they are much better before the main fix than after.
    
    This patch (of 11):
    
    page_vma_mapped_walk() cleanup: sometimes the local copy of pvwm->page
    was used, sometimes pvmw->page itself: use the local copy "page"
    throughout.
    
    Link: https://lkml.kernel.org/r/589b358c-febc-c88e-d4c2-7834b37fa7bf@google.com
    Link: https://lkml.kernel.org/r/88e67645-f467-c279-bf5e-af4b5c6b13eb@google.com
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reviewed-by: Alistair Popple <apopple@nvidia.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit e17afb6d0b7c74f316dbff33588210190600efc7
Author: Yang Shi <shy828301@gmail.com>
Date:   Tue Jun 15 18:24:07 2021 -0700

    mm: thp: replace DEBUG_VM BUG with VM_WARN when unmap fails for split
    
    [ Upstream commit 504e070dc08f757bccaed6d05c0f53ecbfac8a23 ]
    
    When debugging the bug reported by Wang Yugui [1], try_to_unmap() may
    fail, but the first VM_BUG_ON_PAGE() just checks page_mapcount() however
    it may miss the failure when head page is unmapped but other subpage is
    mapped.  Then the second DEBUG_VM BUG() that check total mapcount would
    catch it.  This may incur some confusion.
    
    As this is not a fatal issue, so consolidate the two DEBUG_VM checks
    into one VM_WARN_ON_ONCE_PAGE().
    
    [1] https://lore.kernel.org/linux-mm/20210412180659.B9E3.409509F4@e16-tech.com/
    
    Link: https://lkml.kernel.org/r/d0f0db68-98b8-ebfb-16dc-f29df24cf012@google.com
    Signed-off-by: Yang Shi <shy828301@gmail.com>
    Reviewed-by: Zi Yan <ziy@nvidia.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jue Wang <juew@google.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: fixed up variables and split_queue_lock in
    split_huge_page_to_list(), and conflict on ttu_flags in unmap_page().
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d5cd96a7880322692d64fbe75d321ccd39392537
Author: Hugh Dickins <hughd@google.com>
Date:   Tue Jun 15 18:24:03 2021 -0700

    mm/thp: unmap_mapping_page() to fix THP truncate_cleanup_page()
    
    [ Upstream commit 22061a1ffabdb9c3385de159c5db7aac3a4df1cc ]
    
    There is a race between THP unmapping and truncation, when truncate sees
    pmd_none() and skips the entry, after munmap's zap_huge_pmd() cleared
    it, but before its page_remove_rmap() gets to decrement
    compound_mapcount: generating false "BUG: Bad page cache" reports that
    the page is still mapped when deleted.  This commit fixes that, but not
    in the way I hoped.
    
    The first attempt used try_to_unmap(page, TTU_SYNC|TTU_IGNORE_MLOCK)
    instead of unmap_mapping_range() in truncate_cleanup_page(): it has
    often been an annoyance that we usually call unmap_mapping_range() with
    no pages locked, but there apply it to a single locked page.
    try_to_unmap() looks more suitable for a single locked page.
    
    However, try_to_unmap_one() contains a VM_BUG_ON_PAGE(!pvmw.pte,page):
    it is used to insert THP migration entries, but not used to unmap THPs.
    Copy zap_huge_pmd() and add THP handling now? Perhaps, but their TLB
    needs are different, I'm too ignorant of the DAX cases, and couldn't
    decide how far to go for anon+swap.  Set that aside.
    
    The second attempt took a different tack: make no change in truncate.c,
    but modify zap_huge_pmd() to insert an invalidated huge pmd instead of
    clearing it initially, then pmd_clear() between page_remove_rmap() and
    unlocking at the end.  Nice.  But powerpc blows that approach out of the
    water, with its serialize_against_pte_lookup(), and interesting pgtable
    usage.  It would need serious help to get working on powerpc (with a
    minor optimization issue on s390 too).  Set that aside.
    
    Just add an "if (page_mapped(page)) synchronize_rcu();" or other such
    delay, after unmapping in truncate_cleanup_page()? Perhaps, but though
    that's likely to reduce or eliminate the number of incidents, it would
    give less assurance of whether we had identified the problem correctly.
    
    This successful iteration introduces "unmap_mapping_page(page)" instead
    of try_to_unmap(), and goes the usual unmap_mapping_range_tree() route,
    with an addition to details.  Then zap_pmd_range() watches for this
    case, and does spin_unlock(pmd_lock) if so - just like
    page_vma_mapped_walk() now does in the PVMW_SYNC case.  Not pretty, but
    safe.
    
    Note that unmap_mapping_page() is doing a VM_BUG_ON(!PageLocked) to
    assert its interface; but currently that's only used to make sure that
    page->mapping is stable, and zap_pmd_range() doesn't care if the page is
    locked or not.  Along these lines, in invalidate_inode_pages2_range()
    move the initial unmap_mapping_range() out from under page lock, before
    then calling unmap_mapping_page() under page lock if still mapped.
    
    Link: https://lkml.kernel.org/r/a2a4a148-cdd8-942c-4ef8-51b77f643dbe@google.com
    Fixes: fc127da085c2 ("truncate: handle file thp")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Yang Shi <shy828301@gmail.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jue Wang <juew@google.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: fixed up call to truncate_cleanup_page()
    in truncate_inode_pages_range().  Use hpage_nr_pages() in
    unmap_mapping_page().
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2c595b67fceda4a345ac4dcbd0caa2049fd17cb7
Author: Jue Wang <juew@google.com>
Date:   Tue Jun 15 18:24:00 2021 -0700

    mm/thp: fix page_address_in_vma() on file THP tails
    
    [ Upstream commit 31657170deaf1d8d2f6a1955fbc6fa9d228be036 ]
    
    Anon THP tails were already supported, but memory-failure may need to
    use page_address_in_vma() on file THP tails, which its page->mapping
    check did not permit: fix it.
    
    hughd adds: no current usage is known to hit the issue, but this does
    fix a subtle trap in a general helper: best fixed in stable sooner than
    later.
    
    Link: https://lkml.kernel.org/r/a0d9b53-bf5d-8bab-ac5-759dc61819c1@google.com
    Fixes: 800d8c63b2e9 ("shmem: add huge pages support")
    Signed-off-by: Jue Wang <juew@google.com>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Reviewed-by: Yang Shi <shy828301@gmail.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f38722557339bd22a5f1e8bae013f81b69309e9a
Author: Hugh Dickins <hughd@google.com>
Date:   Tue Jun 15 18:23:56 2021 -0700

    mm/thp: fix vma_address() if virtual address below file offset
    
    [ Upstream commit 494334e43c16d63b878536a26505397fce6ff3a2 ]
    
    Running certain tests with a DEBUG_VM kernel would crash within hours,
    on the total_mapcount BUG() in split_huge_page_to_list(), while trying
    to free up some memory by punching a hole in a shmem huge page: split's
    try_to_unmap() was unable to find all the mappings of the page (which,
    on a !DEBUG_VM kernel, would then keep the huge page pinned in memory).
    
    When that BUG() was changed to a WARN(), it would later crash on the
    VM_BUG_ON_VMA(end < vma->vm_start || start >= vma->vm_end, vma) in
    mm/internal.h:vma_address(), used by rmap_walk_file() for
    try_to_unmap().
    
    vma_address() is usually correct, but there's a wraparound case when the
    vm_start address is unusually low, but vm_pgoff not so low:
    vma_address() chooses max(start, vma->vm_start), but that decides on the
    wrong address, because start has become almost ULONG_MAX.
    
    Rewrite vma_address() to be more careful about vm_pgoff; move the
    VM_BUG_ON_VMA() out of it, returning -EFAULT for errors, so that it can
    be safely used from page_mapped_in_vma() and page_address_in_vma() too.
    
    Add vma_address_end() to apply similar care to end address calculation,
    in page_vma_mapped_walk() and page_mkclean_one() and try_to_unmap_one();
    though it raises a question of whether callers would do better to supply
    pvmw->end to page_vma_mapped_walk() - I chose not, for a smaller patch.
    
    An irritation is that their apparent generality breaks down on KSM
    pages, which cannot be located by the page->index that page_to_pgoff()
    uses: as commit 4b0ece6fa016 ("mm: migrate: fix remove_migration_pte()
    for ksm pages") once discovered.  I dithered over the best thing to do
    about that, and have ended up with a VM_BUG_ON_PAGE(PageKsm) in both
    vma_address() and vma_address_end(); though the only place in danger of
    using it on them was try_to_unmap_one().
    
    Sidenote: vma_address() and vma_address_end() now use compound_nr() on a
    head page, instead of thp_size(): to make the right calculation on a
    hugetlbfs page, whether or not THPs are configured.  try_to_unmap() is
    used on hugetlbfs pages, but perhaps the wrong calculation never
    mattered.
    
    Link: https://lkml.kernel.org/r/caf1c1a3-7cfb-7f8f-1beb-ba816e932825@google.com
    Fixes: a8fa41ad2f6f ("mm, rmap: check all VMAs that PTE-mapped THP can be part of")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jue Wang <juew@google.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: fixed up conflicts on intervening thp_size(),
    and mmu_notifier_range initializations; substitute for compound_nr().
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 205899d6be9a1d5494318920a211852a32886e46
Author: Hugh Dickins <hughd@google.com>
Date:   Tue Jun 15 18:23:53 2021 -0700

    mm/thp: try_to_unmap() use TTU_SYNC for safe splitting
    
    [ Upstream commit 732ed55823fc3ad998d43b86bf771887bcc5ec67 ]
    
    Stressing huge tmpfs often crashed on unmap_page()'s VM_BUG_ON_PAGE
    (!unmap_success): with dump_page() showing mapcount:1, but then its raw
    struct page output showing _mapcount ffffffff i.e.  mapcount 0.
    
    And even if that particular VM_BUG_ON_PAGE(!unmap_success) is removed,
    it is immediately followed by a VM_BUG_ON_PAGE(compound_mapcount(head)),
    and further down an IS_ENABLED(CONFIG_DEBUG_VM) total_mapcount BUG():
    all indicative of some mapcount difficulty in development here perhaps.
    But the !CONFIG_DEBUG_VM path handles the failures correctly and
    silently.
    
    I believe the problem is that once a racing unmap has cleared pte or
    pmd, try_to_unmap_one() may skip taking the page table lock, and emerge
    from try_to_unmap() before the racing task has reached decrementing
    mapcount.
    
    Instead of abandoning the unsafe VM_BUG_ON_PAGE(), and the ones that
    follow, use PVMW_SYNC in try_to_unmap_one() in this case: adding
    TTU_SYNC to the options, and passing that from unmap_page().
    
    When CONFIG_DEBUG_VM, or for non-debug too? Consensus is to do the same
    for both: the slight overhead added should rarely matter, except perhaps
    if splitting sparsely-populated multiply-mapped shmem.  Once confident
    that bugs are fixed, TTU_SYNC here can be removed, and the race
    tolerated.
    
    Link: https://lkml.kernel.org/r/c1e95853-8bcd-d8fd-55fa-e7f2488e78f@google.com
    Fixes: fec89c109f3a ("thp: rewrite freeze_page()/unfreeze_page() with generic rmap walkers")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jue Wang <juew@google.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: upstream TTU_SYNC 0x10 takes the value which
    5.11 commit 013339df116c ("mm/rmap: always do TTU_IGNORE_ACCESS") freed.
    It is very tempting to backport that commit (as 5.10 already did) and
    make no change here; but on reflection, good as that commit is, I'm
    reluctant to include any possible side-effect of it in this series.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit fc1fbc5b017b6f5ef24a4a93f33cd022225e01c8
Author: Hugh Dickins <hughd@google.com>
Date:   Tue Jun 15 18:23:49 2021 -0700

    mm/thp: make is_huge_zero_pmd() safe and quicker
    
    [ Upstream commit 3b77e8c8cde581dadab9a0f1543a347e24315f11 ]
    
    Most callers of is_huge_zero_pmd() supply a pmd already verified
    present; but a few (notably zap_huge_pmd()) do not - it might be a pmd
    migration entry, in which the pfn is encoded differently from a present
    pmd: which might pass the is_huge_zero_pmd() test (though not on x86,
    since L1TF forced us to protect against that); or perhaps even crash in
    pmd_page() applied to a swap-like entry.
    
    Make it safe by adding pmd_present() check into is_huge_zero_pmd()
    itself; and make it quicker by saving huge_zero_pfn, so that
    is_huge_zero_pmd() will not need to do that pmd_page() lookup each time.
    
    __split_huge_pmd_locked() checked pmd_trans_huge() before: that worked,
    but is unnecessary now that is_huge_zero_pmd() checks present.
    
    Link: https://lkml.kernel.org/r/21ea9ca-a1f5-8b90-5e88-95fb1c49bbfa@google.com
    Fixes: e71769ae5260 ("mm: enable thp migration for shmem thp")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Yang Shi <shy828301@gmail.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jue Wang <juew@google.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 629ee482e0966c23ddc1abeebf71aa84ff8d3673
Author: Hugh Dickins <hughd@google.com>
Date:   Tue Jun 15 18:23:45 2021 -0700

    mm/thp: fix __split_huge_pmd_locked() on shmem migration entry
    
    [ Upstream commit 99fa8a48203d62b3743d866fc48ef6abaee682be ]
    
    Patch series "mm/thp: fix THP splitting unmap BUGs and related", v10.
    
    Here is v2 batch of long-standing THP bug fixes that I had not got
    around to sending before, but prompted now by Wang Yugui's report
    https://lore.kernel.org/linux-mm/20210412180659.B9E3.409509F4@e16-tech.com/
    
    Wang Yugui has tested a rollup of these fixes applied to 5.10.39, and
    they have done no harm, but have *not* fixed that issue: something more
    is needed and I have no idea of what.
    
    This patch (of 7):
    
    Stressing huge tmpfs page migration racing hole punch often crashed on
    the VM_BUG_ON(!pmd_present) in pmdp_huge_clear_flush(), with DEBUG_VM=y
    kernel; or shortly afterwards, on a bad dereference in
    __split_huge_pmd_locked() when DEBUG_VM=n.  They forgot to allow for pmd
    migration entries in the non-anonymous case.
    
    Full disclosure: those particular experiments were on a kernel with more
    relaxed mmap_lock and i_mmap_rwsem locking, and were not repeated on the
    vanilla kernel: it is conceivable that stricter locking happens to avoid
    those cases, or makes them less likely; but __split_huge_pmd_locked()
    already allowed for pmd migration entries when handling anonymous THPs,
    so this commit brings the shmem and file THP handling into line.
    
    And while there: use old_pmd rather than _pmd, as in the following
    blocks; and make it clearer to the eye that the !vma_is_anonymous()
    block is self-contained, making an early return after accounting for
    unmapping.
    
    Link: https://lkml.kernel.org/r/af88612-1473-2eaa-903-8d1a448b26@google.com
    Link: https://lkml.kernel.org/r/dd221a99-efb3-cd1d-6256-7e646af29314@google.com
    Fixes: e71769ae5260 ("mm: enable thp migration for shmem thp")
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Wang Yugui <wangyugui@e16-tech.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Naoya Horiguchi <naoya.horiguchi@nec.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Ralph Campbell <rcampbell@nvidia.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Jue Wang <juew@google.com>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: this commit made intervening cleanups in
    pmdp_huge_clear_flush() redundant: here it's rediffed to skip them.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 420afb489a1f6981cbe2fb9ee10007dba878e223
Author: Miaohe Lin <linmiaohe@huawei.com>
Date:   Thu Feb 25 17:18:03 2021 -0800

    mm/rmap: use page_not_mapped in try_to_unmap()
    
    [ Upstream commit b7e188ec98b1644ff70a6d3624ea16aadc39f5e0 ]
    
    page_mapcount_is_zero() calculates accurately how many mappings a hugepage
    has in order to check against 0 only.  This is a waste of cpu time.  We
    can do this via page_not_mapped() to save some possible atomic_read
    cycles.  Remove the function page_mapcount_is_zero() as it's not used
    anymore and move page_not_mapped() above try_to_unmap() to avoid
    identifier undeclared compilation error.
    
    Link: https://lkml.kernel.org/r/20210130084904.35307-1-linmiaohe@huawei.com
    Signed-off-by: Miaohe Lin <linmiaohe@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 784445344c9e7cdef2a2a724fab34243933a91dc
Author: Miaohe Lin <linmiaohe@huawei.com>
Date:   Thu Feb 25 17:17:56 2021 -0800

    mm/rmap: remove unneeded semicolon in page_not_mapped()
    
    [ Upstream commit e0af87ff7afcde2660be44302836d2d5618185af ]
    
    Remove extra semicolon without any functional change intended.
    
    Link: https://lkml.kernel.org/r/20210127093425.39640-1-linmiaohe@huawei.com
    Signed-off-by: Miaohe Lin <linmiaohe@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 131cb7a08c767e2d082ff5de6d384d8d935fb6f1
Author: Alex Shi <alexs@kernel.org>
Date:   Fri Dec 18 14:01:31 2020 -0800

    mm: add VM_WARN_ON_ONCE_PAGE() macro
    
    [ Upstream commit a4055888629bc0467d12d912cd7c90acdf3d9b12 part ]
    
    Add VM_WARN_ON_ONCE_PAGE() macro.
    
    Link: https://lkml.kernel.org/r/1604283436-18880-3-git-send-email-alex.shi@linux.alibaba.com
    Signed-off-by: Alex Shi <alex.shi@linux.alibaba.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Note on stable backport: original commit was titled
    mm/memcg: warning on !memcg after readahead page charged
    which included uses of this macro in mm/memcontrol.c: here omitted.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
