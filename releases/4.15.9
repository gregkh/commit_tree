commit 3eae9e93d49241dfb30be1d706b68d056b1ad29c
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sun Mar 11 16:25:15 2018 +0100

    Linux 4.15.9

commit c0d3435d7fa07cd4720e632919ec1a4eb4147621
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 1 22:16:21 2018 +0100

    KVM: x86: fix backward migration with async_PF
    
    commit fe2a3027e74e40a3ece3a4c1e4e51403090a907a upstream.
    
    Guests on new hypersiors might set KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT
    bit when enabling async_PF, but this bit is reserved on old hypervisors,
    which results in a failure upon migration.
    
    To avoid breaking different cases, we are checking for CPUID feature bit
    before enabling the feature and nothing else.
    
    Fixes: 52a5c155cf79 ("KVM: async_pf: Let guest support delivery of async_pf from guest mode")
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Wanpeng Li <wanpengli@tencent.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [jwang: port to 4.14]
    Signed-off-by: Jack Wang <jinpu.wang@profitbricks.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fa1f98c84c6b275b596486029cc6293967c6a6ac
Author: Sreekanth Reddy <sreekanth.reddy@broadcom.com>
Date:   Fri Feb 16 20:39:58 2018 -0200

    scsi: mpt3sas: wait for and flush running commands on shutdown/unload
    
    commit c666d3be99c000bb889a33353e9be0fa5808d3de upstream.
    
    This patch finishes all outstanding SCSI IO commands (but not other commands,
    e.g., task management) in the shutdown and unload paths.
    
    It first waits for the commands to complete (this is done after setting
    'ioc->remove_host = 1 ', which prevents new commands to be queued) then it
    flushes commands that might still be running.
    
    This avoids triggering error handling (e.g., abort command) for all commands
    possibly completed by the adapter after interrupts disabled.
    
    [mauricfo: introduced something in commit message.]
    
    Signed-off-by: Sreekanth Reddy <sreekanth.reddy@broadcom.com>
    Tested-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    [mauricfo: backport to linux-4.15.y (a few updates to context lines)]
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2928c03a245fe61667aa4e886e662d8bf674c585
Author: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
Date:   Fri Feb 16 20:39:57 2018 -0200

    scsi: mpt3sas: fix oops in error handlers after shutdown/unload
    
    commit 9ff549ffb4fb4cc9a4b24d1de9dc3e68287797c4 upstream.
    
    This patch adds checks for 'ioc->remove_host' in the SCSI error handlers, so
    not to access pointers/resources potentially freed in the PCI shutdown/module
    unload path.  The error handlers may be invoked after shutdown/unload,
    depending on other components.
    
    This problem was observed with kexec on a system with a mpt3sas based adapter
    and an infiniband adapter which takes long enough to shutdown:
    
    The mpt3sas driver finished shutting down / disabled interrupt handling, thus
    some commands have not finished and timed out.
    
    Since the system was still running (waiting for the infiniband adapter to
    shutdown), the scsi error handler for task abort of mpt3sas was invoked, and
    hit an oops -- either in scsih_abort() because 'ioc->scsi_lookup' was NULL
    without commit dbec4c9040ed ("scsi: mpt3sas: lockless command submission"), or
    later up in scsih_host_reset() (with or without that commit), because it
    eventually called mpt3sas_base_get_iocstate().
    
    After the above commit, the oops in scsih_abort() does not occur anymore
    (_scsih_scsi_lookup_find_by_scmd() is no longer called), but that commit is
    too big and out of the scope of linux-stable, where this patch might help, so
    still go for the changes.
    
    Also, this might help to prevent similar errors in the future, in case code
    changes and possibly tries to access freed stuff.
    
    Note the fix in scsih_host_reset() is still important anyway.
    
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Acked-by: Sreekanth Reddy <Sreekanth.Reddy@broadcom.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6ff2082e454407c203ba1468b13da274fc5a9f33
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:16:49 2018 +0100

    bpf, ppc64: fix out of bounds access in tail call
    
    [ upstream commit d269176e766c71c998cb75b4ea8cbc321cc0019d ]
    
    While working on 16338a9b3ac3 ("bpf, arm64: fix out of bounds access in
    tail call") I noticed that ppc64 JIT is partially affected as well. While
    the bound checking is correctly performed as unsigned comparison, the
    register with the index value however, is never truncated into 32 bit
    space, so e.g. a index value of 0x100000000ULL with a map of 1 element
    would pass with PPC_CMPLW() whereas we later on continue with the full
    64 bit register value. Therefore, as we do in interpreter and other JITs
    truncate the value to 32 bit initially in order to fix access.
    
    Fixes: ce0761419fae ("powerpc/bpf: Implement support for tail calls")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Tested-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bc9d150b9bf1981c8adc6948537e2b80c0e58761
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:16:48 2018 +0100

    bpf: allow xadd only on aligned memory
    
    [ upstream commit ca36960211eb228bcbc7aaebfa0d027368a94c60 ]
    
    The requirements around atomic_add() / atomic64_add() resp. their
    JIT implementations differ across architectures. E.g. while x86_64
    seems just fine with BPF's xadd on unaligned memory, on arm64 it
    triggers via interpreter but also JIT the following crash:
    
      [  830.864985] Unable to handle kernel paging request at virtual address ffff8097d7ed6703
      [...]
      [  830.916161] Internal error: Oops: 96000021 [#1] SMP
      [  830.984755] CPU: 37 PID: 2788 Comm: test_verifier Not tainted 4.16.0-rc2+ #8
      [  830.991790] Hardware name: Huawei TaiShan 2280 /BC11SPCD, BIOS 1.29 07/17/2017
      [  830.998998] pstate: 80400005 (Nzcv daif +PAN -UAO)
      [  831.003793] pc : __ll_sc_atomic_add+0x4/0x18
      [  831.008055] lr : ___bpf_prog_run+0x1198/0x1588
      [  831.012485] sp : ffff00001ccabc20
      [  831.015786] x29: ffff00001ccabc20 x28: ffff8017d56a0f00
      [  831.021087] x27: 0000000000000001 x26: 0000000000000000
      [  831.026387] x25: 000000c168d9db98 x24: 0000000000000000
      [  831.031686] x23: ffff000008203878 x22: ffff000009488000
      [  831.036986] x21: ffff000008b14e28 x20: ffff00001ccabcb0
      [  831.042286] x19: ffff0000097b5080 x18: 0000000000000a03
      [  831.047585] x17: 0000000000000000 x16: 0000000000000000
      [  831.052885] x15: 0000ffffaeca8000 x14: 0000000000000000
      [  831.058184] x13: 0000000000000000 x12: 0000000000000000
      [  831.063484] x11: 0000000000000001 x10: 0000000000000000
      [  831.068783] x9 : 0000000000000000 x8 : 0000000000000000
      [  831.074083] x7 : 0000000000000000 x6 : 000580d428000000
      [  831.079383] x5 : 0000000000000018 x4 : 0000000000000000
      [  831.084682] x3 : ffff00001ccabcb0 x2 : 0000000000000001
      [  831.089982] x1 : ffff8097d7ed6703 x0 : 0000000000000001
      [  831.095282] Process test_verifier (pid: 2788, stack limit = 0x0000000018370044)
      [  831.102577] Call trace:
      [  831.105012]  __ll_sc_atomic_add+0x4/0x18
      [  831.108923]  __bpf_prog_run32+0x4c/0x70
      [  831.112748]  bpf_test_run+0x78/0xf8
      [  831.116224]  bpf_prog_test_run_xdp+0xb4/0x120
      [  831.120567]  SyS_bpf+0x77c/0x1110
      [  831.123873]  el0_svc_naked+0x30/0x34
      [  831.127437] Code: 97fffe97 17ffffec 00000000 f9800031 (885f7c31)
    
    Reason for this is because memory is required to be aligned. In
    case of BPF, we always enforce alignment in terms of stack access,
    but not when accessing map values or packet data when the underlying
    arch (e.g. arm64) has CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS set.
    
    xadd on packet data that is local to us anyway is just wrong, so
    forbid this case entirely. The only place where xadd makes sense in
    fact are map values; xadd on stack is wrong as well, but it's been
    around for much longer. Specifically enforce strict alignment in case
    of xadd, so that we handle this case generically and avoid such crashes
    in the first place.
    
    Fixes: 17a5267067f3 ("bpf: verifier (add verifier core)")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8c4626bf15bad3f8b6cde2848408b8d3f96fccf0
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 8 13:16:47 2018 +0100

    bpf: add schedule points in percpu arrays management
    
    [ upstream commit 32fff239de37ef226d5b66329dd133f64d63b22d ]
    
    syszbot managed to trigger RCU detected stalls in
    bpf_array_free_percpu()
    
    It takes time to allocate a huge percpu map, but even more time to free
    it.
    
    Since we run in process context, use cond_resched() to yield cpu if
    needed.
    
    Fixes: a10423b87a7e ("bpf: introduce BPF_MAP_TYPE_PERCPU_ARRAY map")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 18fa1b102cc2198d43e89e55dfdf3dd8d02e101c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:16:46 2018 +0100

    bpf, arm64: fix out of bounds access in tail call
    
    [ upstream commit 16338a9b3ac30740d49f5dfed81bac0ffa53b9c7 ]
    
    I recently noticed a crash on arm64 when feeding a bogus index
    into BPF tail call helper. The crash would not occur when the
    interpreter is used, but only in case of JIT. Output looks as
    follows:
    
      [  347.007486] Unable to handle kernel paging request at virtual address fffb850e96492510
      [...]
      [  347.043065] [fffb850e96492510] address between user and kernel address ranges
      [  347.050205] Internal error: Oops: 96000004 [#1] SMP
      [...]
      [  347.190829] x13: 0000000000000000 x12: 0000000000000000
      [  347.196128] x11: fffc047ebe782800 x10: ffff808fd7d0fd10
      [  347.201427] x9 : 0000000000000000 x8 : 0000000000000000
      [  347.206726] x7 : 0000000000000000 x6 : 001c991738000000
      [  347.212025] x5 : 0000000000000018 x4 : 000000000000ba5a
      [  347.217325] x3 : 00000000000329c4 x2 : ffff808fd7cf0500
      [  347.222625] x1 : ffff808fd7d0fc00 x0 : ffff808fd7cf0500
      [  347.227926] Process test_verifier (pid: 4548, stack limit = 0x000000007467fa61)
      [  347.235221] Call trace:
      [  347.237656]  0xffff000002f3a4fc
      [  347.240784]  bpf_test_run+0x78/0xf8
      [  347.244260]  bpf_prog_test_run_skb+0x148/0x230
      [  347.248694]  SyS_bpf+0x77c/0x1110
      [  347.251999]  el0_svc_naked+0x30/0x34
      [  347.255564] Code: 9100075a d280220a 8b0a002a d37df04b (f86b694b)
      [...]
    
    In this case the index used in BPF r3 is the same as in r1
    at the time of the call, meaning we fed a pointer as index;
    here, it had the value 0xffff808fd7cf0500 which sits in x2.
    
    While I found tail calls to be working in general (also for
    hitting the error cases), I noticed the following in the code
    emission:
    
      # bpftool p d j i 988
      [...]
      38:   ldr     w10, [x1,x10]
      3c:   cmp     w2, w10
      40:   b.ge    0x000000000000007c              <-- signed cmp
      44:   mov     x10, #0x20                      // #32
      48:   cmp     x26, x10
      4c:   b.gt    0x000000000000007c
      50:   add     x26, x26, #0x1
      54:   mov     x10, #0x110                     // #272
      58:   add     x10, x1, x10
      5c:   lsl     x11, x2, #3
      60:   ldr     x11, [x10,x11]                  <-- faulting insn (f86b694b)
      64:   cbz     x11, 0x000000000000007c
      [...]
    
    Meaning, the tests passed because commit ddb55992b04d ("arm64:
    bpf: implement bpf_tail_call() helper") was using signed compares
    instead of unsigned which as a result had the test wrongly passing.
    
    Change this but also the tail call count test both into unsigned
    and cap the index as u32. Latter we did as well in 90caccdd8cc0
    ("bpf: fix bpf_tail_call() x64 JIT") and is needed in addition here,
    too. Tested on HiSilicon Hi1616.
    
    Result after patch:
    
      # bpftool p d j i 268
      [...]
      38:   ldr     w10, [x1,x10]
      3c:   add     w2, w2, #0x0
      40:   cmp     w2, w10
      44:   b.cs    0x0000000000000080
      48:   mov     x10, #0x20                      // #32
      4c:   cmp     x26, x10
      50:   b.hi    0x0000000000000080
      54:   add     x26, x26, #0x1
      58:   mov     x10, #0x110                     // #272
      5c:   add     x10, x1, x10
      60:   lsl     x11, x2, #3
      64:   ldr     x11, [x10,x11]
      68:   cbz     x11, 0x0000000000000080
      [...]
    
    Fixes: ddb55992b04d ("arm64: bpf: implement bpf_tail_call() helper")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5882764e48ed6275c5880678bb135ef39e7be959
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:16:45 2018 +0100

    bpf, x64: implement retpoline for tail call
    
    [ upstream commit a493a87f38cfa48caaa95c9347be2d914c6fdf29 ]
    
    Implement a retpoline [0] for the BPF tail call JIT'ing that converts
    the indirect jump via jmp %rax that is used to make the long jump into
    another JITed BPF image. Since this is subject to speculative execution,
    we need to control the transient instruction sequence here as well
    when CONFIG_RETPOLINE is set, and direct it into a pause + lfence loop.
    The latter aligns also with what gcc / clang emits (e.g. [1]).
    
    JIT dump after patch:
    
      # bpftool p d x i 1
       0: (18) r2 = map[id:1]
       2: (b7) r3 = 0
       3: (85) call bpf_tail_call#12
       4: (b7) r0 = 2
       5: (95) exit
    
    With CONFIG_RETPOLINE:
    
      # bpftool p d j i 1
      [...]
      33:   cmp    %edx,0x24(%rsi)
      36:   jbe    0x0000000000000072  |*
      38:   mov    0x24(%rbp),%eax
      3e:   cmp    $0x20,%eax
      41:   ja     0x0000000000000072  |
      43:   add    $0x1,%eax
      46:   mov    %eax,0x24(%rbp)
      4c:   mov    0x90(%rsi,%rdx,8),%rax
      54:   test   %rax,%rax
      57:   je     0x0000000000000072  |
      59:   mov    0x28(%rax),%rax
      5d:   add    $0x25,%rax
      61:   callq  0x000000000000006d  |+
      66:   pause                      |
      68:   lfence                     |
      6b:   jmp    0x0000000000000066  |
      6d:   mov    %rax,(%rsp)         |
      71:   retq                       |
      72:   mov    $0x2,%eax
      [...]
    
      * relative fall-through jumps in error case
      + retpoline for indirect jump
    
    Without CONFIG_RETPOLINE:
    
      # bpftool p d j i 1
      [...]
      33:   cmp    %edx,0x24(%rsi)
      36:   jbe    0x0000000000000063  |*
      38:   mov    0x24(%rbp),%eax
      3e:   cmp    $0x20,%eax
      41:   ja     0x0000000000000063  |
      43:   add    $0x1,%eax
      46:   mov    %eax,0x24(%rbp)
      4c:   mov    0x90(%rsi,%rdx,8),%rax
      54:   test   %rax,%rax
      57:   je     0x0000000000000063  |
      59:   mov    0x28(%rax),%rax
      5d:   add    $0x25,%rax
      61:   jmpq   *%rax               |-
      63:   mov    $0x2,%eax
      [...]
    
      * relative fall-through jumps in error case
      - plain indirect jump as before
    
      [0] https://support.google.com/faqs/answer/7625886
      [1] https://github.com/gcc-mirror/gcc/commit/a31e654fa107be968b802786d747e962c2fcdb2b
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 519f40bb7fc98c0e7ac2f8f958ef3c3b8a5d1404
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Mar 8 13:16:44 2018 +0100

    bpf: fix rcu lockdep warning for lpm_trie map_free callback
    
    [ upstream commit 6c5f61023c5b0edb0c8a64c902fe97c6453b1852 ]
    
    Commit 9a3efb6b661f ("bpf: fix memory leak in lpm_trie map_free callback function")
    fixed a memory leak and removed unnecessary locks in map_free callback function.
    Unfortrunately, it introduced a lockdep warning. When lockdep checking is turned on,
    running tools/testing/selftests/bpf/test_lpm_map will have:
    
      [   98.294321] =============================
      [   98.294807] WARNING: suspicious RCU usage
      [   98.295359] 4.16.0-rc2+ #193 Not tainted
      [   98.295907] -----------------------------
      [   98.296486] /home/yhs/work/bpf/kernel/bpf/lpm_trie.c:572 suspicious rcu_dereference_check() usage!
      [   98.297657]
      [   98.297657] other info that might help us debug this:
      [   98.297657]
      [   98.298663]
      [   98.298663] rcu_scheduler_active = 2, debug_locks = 1
      [   98.299536] 2 locks held by kworker/2:1/54:
      [   98.300152]  #0:  ((wq_completion)"events"){+.+.}, at: [<00000000196bc1f0>] process_one_work+0x157/0x5c0
      [   98.301381]  #1:  ((work_completion)(&map->work)){+.+.}, at: [<00000000196bc1f0>] process_one_work+0x157/0x5c0
    
    Since actual trie tree removal happens only after no other
    accesses to the tree are possible, replacing
      rcu_dereference_protected(*slot, lockdep_is_held(&trie->lock))
    with
      rcu_dereference_protected(*slot, 1)
    fixed the issue.
    
    Fixes: 9a3efb6b661f ("bpf: fix memory leak in lpm_trie map_free callback function")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f154de29a46b7498f957c7b22528571eb8e11211
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Mar 8 13:16:43 2018 +0100

    bpf: fix memory leak in lpm_trie map_free callback function
    
    [ upstream commit 9a3efb6b661f71d5675369ace9257833f0e78ef3 ]
    
    There is a memory leak happening in lpm_trie map_free callback
    function trie_free. The trie structure itself does not get freed.
    
    Also, trie_free function did not do synchronize_rcu before freeing
    various data structures. This is incorrect as some rcu_read_lock
    region(s) for lookup, update, delete or get_next_key may not complete yet.
    The fix is to add synchronize_rcu in the beginning of trie_free.
    The useless spin_lock is removed from this function as well.
    
    Fixes: b95a5c4db09b ("bpf: add a longest prefix match trie map implementation")
    Reported-by: Mathieu Malaterre <malat@debian.org>
    Reported-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit da43a222a7593b1336d680f15fe2867b7bf6a085
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 8 13:16:42 2018 +0100

    bpf: fix mlock precharge on arraymaps
    
    [ upstream commit 9c2d63b843a5c8a8d0559cc067b5398aa5ec3ffc ]
    
    syzkaller recently triggered OOM during percpu map allocation;
    while there is work in progress by Dennis Zhou to add __GFP_NORETRY
    semantics for percpu allocator under pressure, there seems also a
    missing bpf_map_precharge_memlock() check in array map allocation.
    
    Given today the actual bpf_map_charge_memlock() happens after the
    find_and_alloc_map() in syscall path, the bpf_map_precharge_memlock()
    is there to bail out early before we go and do the map setup work
    when we find that we hit the limits anyway. Therefore add this for
    array map as well.
    
    Fixes: 6c9059817432 ("bpf: pre-allocate hash map elements")
    Fixes: a10423b87a7e ("bpf: introduce BPF_MAP_TYPE_PERCPU_ARRAY map")
    Reported-by: syzbot+adb03f3f0bb57ce3acda@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Dennis Zhou <dennisszhou@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
